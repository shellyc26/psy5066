<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Sampling Distributions</title>
    <meta charset="utf-8" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/rladies.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/rladies-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Sampling Distributions

---




## What do we want?

We want to make inferences about a population

  - But the population is too large to measure directly
  - So we need to estimate the population parameters
  
---

## Population

- The population distribution is a *theoretical probability distribution* that has some mathematical form

- We want ot know how the population distribution influences the distribution of **sample** statistics

- Ultimately we want to use a sample distribution to understand the population distribution

---

## What is the *point* of inferential stats?

Point estimation: we use our sample statistics to take our best guess of the population parameter

We know that our estimates will vary from sample to sample

We're using our sample as an estimate
  - Sample mean `\(\bar{X}\)` is an **_estimator_**
  - A specific sample is an **_estiamte_**

---

## Population vs. Sample

|             | Population&lt;br&gt;Distribution |  Sample&lt;br&gt;Distribution |
|:-----------:|:-----------:|:-----------:|:-----------:|
| Distiribution consists of:    |    Individual observations&lt;br&gt; `\(x\)`    | Individual observations&lt;br&gt; `\(x\)`       | 
| Central tendency |    `\(\mu\)`   | `\(\bar{x}\)`      | 
| Dispersion | `\(\sigma^2\)` | `\(s^2\)` |
|            | `\(\sigma\)` | `\(s\)` |
| Type       | Parameter | Statistic |
| T vs. O    | Theoretical | Observed |


---

## Sampling Distribution

- The major goal that we have in statistical inference is to make confident claims about the *population* based on a small representation of it, the *sample*.

- Any sample will be off the mark in how well it captures the important features of a population. The **sampling distribution** tells us how far off the mark we can expect a sample statistic to be. 


---
## Sampling Distribution
- We use features of the sample (*statistics*) to inform us about features of the population (*parameters*).
- The quality of this information goes up as sample size goes up -- **the Law of Large Numbers**. 

All sample statistics are wrong, but they become more useful as sample size increases. 

So...
  - how big does the sample need to be?
  - for a given sample size, how precise is a sample statistic as a representation of the population?

---
.left-column[
### Population distribution
.small[The parameters of this distribution are unknown.

We use the sample to inform us about the likely characteristics of the population.]

]

![](8-sampling_files/figure-html/unnamed-chunk-1-1.png)&lt;!-- --&gt;

---

.left-column[

### Samples from the population 

.small[
Each *sample distribution* will resemble the population. That resemblance will be better as sample size increases: The Law of Large Numbers.

Statistics (e.g., mean) can be calculated for any sample.
]
]

![](8-sampling_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;

???
here, the sample sizes is 30 fo each of these 4 graphs. each of theses is a sample. and each sample has it's only mean and sd. 
---

.left-column[
.small[
A statistic (e.g., mean) from a large number of samples also have a distribution: the **samplING distribution**.

The mean of the **sampling distribution** converges on `\(\mu\)`

]
]

![](8-sampling_files/figure-html/sampling1-1.png)&lt;!-- --&gt;

---

.left-column[
.small[
This distribution has a standard deviation that tells us how typical or rare values of the sample statistic are likely to be.

The sampling distribution of the mean is of particular interest, it's called the **standard error of the mean** (SEM). 
]
]


![](8-sampling_files/figure-html/sampling2-1.png)&lt;!-- --&gt;

???

Sampling distributions can be constructed around any statistic -- ranges, standard deviations, difference scores. The standard errors of those distributions are also standard errors. (E.g., the standard error of the difference.)

---

![](images/sampling.png)

---
## Sampling Distributions

- Distribution of values of a particular statistic ( `\(\bar{x}\)`, `\(s^2\)`, `\(s\)`) across all possible samples of N observations
  - To keep it simple, let's just focus on the mean 
  - Statistic will be our *estimator* of the population

- **Sampling distriubtion `\(\neq\)` sample distribution**

---
## Sampling Distributions

A theoretical probability distribution of all possible values of some statistic, computed from samples of the same size randomly drawn from the same population

Provides the frequency/probability with which values of statistics are observed or are expected to be observed when random samples of size N are drawn from a given population

---
## Interactive Example

PLAY WITH THIS!

[Sampling distribution example](http://shiny.calpoly.sh/Sampling_Distribution/)

---
## Sampling Distribution Approximates the Normal

One of the most important discoveries in statistics is that the sampling distributions of many statistics are approximately **normal** even when the sample (and population) distributions are not.

  - For example, the mean of a random sample will not precisely equal the population mean. But, how far off will it be? And what distribution shape will these possible sample mean values have?

The error represented by how far off a sample mean is from the population mean is called **sampling error**.  

---
## Central limit theorem

According to the **central limit theorem**, as sample size increases, the sampling distribution of the mean approaches normality, even when the data upon which the mean is based are not normally distributed.

The sample size necessary to be "approximately normal" depends on the nature of the underlying data.  The less normal it is, the larger the sample size necessary in order for the sampling distribution of the means to become normal.

"Around sample size of 30" is a common rule of thumb.
- Note, however, that this rule of thumb is sufficiently only to assume that the sampling distribution is normal. It doesn't magically mean your sample will be normally distributed

???

Regardless of the CLT, if the data are skewed, we might wonder if the mean is the best estimator to use here.

---

.left-column[Ends up that quite a few sample statistics approach normality as sample size increases.

Here is the sample standard deviation from a normal distribution with `\(\sigma = 1\)`.
]

![](8-sampling_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;

---

.left-column[And the range.]

![](8-sampling_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;
???
Note here that we're not just narrowing in, but our mean estimate is getting larger. 

Remember bias?

---

## Relationship Between Population &amp; Sampling

- If the population is normally distributed, the sampling distribution of the mean will be normally disributed

- If the population distribution is not normally distributed, the sampling distribution of the mean will become increasingly normally disributed as sample size increases

- We can use the normal distribution to make inferences about the unknown population mean, based on the sample mean and sample standard deviation

---
## Sampling Mean = Population Mean

$$ E(\bar{x}) = \mu$$

Expected value is the long run average

---
## Sampling Mean = Population Mean
$$
`\begin{equation}
\bar{x} = \frac{(x_1 + x_2 + ... x_N)}{N} \\
E(\bar{x}) = E\bigg(\frac{(x_1 + x_2 + ... x_N)}{N}\bigg) \\
E(\bar{x}) = \frac{E(x_1) + E(x_2) + ... E(x_N)}{E(N)} \\
E(\bar{x}) = \frac{\mu + \mu + ...\mu}{N} \\
E(\bar{x}) = \frac{N\mu}{N} \\
E(\bar{x}) = \mu
\end{equation}`
$$

---
## Expected Value Variance
You can also have a "long run" or expected variance, too. Equivalent to the population variance.

`\(\sigma = E(\bar{x}^2) - (E(\bar{x}))^2\)`

First term:
 - square your `\(x\)` (each score is squared)
 - multiply each value by probability (if 5 scores, 1/5)
 - take the sum

Second term:
 - get the mean of `\(x\)`
 - square it

---
## Sampling SD `\(\neq\)` Population SD

`$$\sigma_M^{2} = E(\bar{x}^2) - (E(\bar{x}))^2$$`

[insert long proof here...]

`$$E(\bar{x}^2) = \frac{\sigma_M^{2}}{N} + \mu^2$$`

---
## Variability in the Sampling Distribution

**Standard Error of the Mean**
  - The standard deviation of the sampling distribution
  - Directly related to the variability of the underlying data:

`$$\sigma_{m} = \frac{\sigma_X}{\sqrt{N}}$$`

- The smaller the SEM, the more likely it is that your sample estimate of the mean will be closer to the population estimate of the mean
- SEM is a function of sample size! The more accurate your sample mean, the closer you are to approximating the population, and the smaller the standard error 

???
We do not know `\(\sigma\)` but we can estimate it based on the sample statistic:

`$$\hat{\sigma} = s = \sqrt{\frac{\sum(x - \bar{x})^2}{N-1}}$$`

---

## More SEM

`$$\hat{\sigma} = s = \sqrt{\frac{\sum(x - \bar{x})^2}{N-1}}$$`

This is the sample estimate of the population standard deviation.  This is an unbiased estimate of `\(\sigma\)` and relies on the sample mean, which is an unbiased estimate of `\(\mu\)`.

This is different from the sample standard deviation, which divides the sum of squares by `\(N\)` rather than `\(N-1\)`.

`$$SEM = \sigma_M = \frac{\hat{\sigma}}{\sqrt{N}} = \frac{\text{Estimate of pop SD}}{\sqrt{N}}$$`
.small[(Most methods of calculating standard deviation assume you're estimating the popluation from a sample and correct for bias.)]

---
## Making Statements

The sampling distribution of means can be used to make probabilistic statements about means in the same way that the standard normal distribution is used to make probabilistic statements about scores.

For example, we can determine the range within which the population mean is likely to be with a particular level of confidence.

Or, we can propose different values for the population mean and ask how typical or rare the sample mean would be if that population value were true.  We can then compare the plausibility of different such “models” of the population.

???
The key is that we have a sampling distribution of the mean with a standard deviation **(the SEM)** that is linked to the population:

---
## Confidence Intervals

The sampling distribution of the mean has variability, represented by the SEM, reflecting uncertainty in the sample mean as an estimate of the population mean.

The assumption of normality allows us to construct an interval within which we have good reason to believe a sample mean will fall if it comes from a particular population:

$$\mu - (1.96\times SEM) \leq \bar{X} \leq \mu + (1.96\times SEM) $$

We can reorganize this to allow statements about the population mean, which is usually not known:

$$\bar{X} - (1.96\times SEM) \leq \mu \leq \bar{X} + (1.96\times SEM) $$
???
Sampling distributions are normally distributed
---
## Confidence Intervals

$$\bar{X} - (1.96\times SEM) \leq \mu \leq \bar{X} + (1.96\times SEM) $$

- This is referred to as the **95% confidence interval (CI)**
- Note the assumption of normality, which should hold by the Central Limit Theorem, if N is sufficiently large.

The 95% CI is sometimes represented as:

`$$CI_{95} = \bar{X} \pm [1.96\frac{\hat{\sigma}}{\sqrt{N}}]$$`


---

.left-column[
.small[
What if N is not “sufficiently large?”  The normal distribution assumes we know the population mean and standard deviation. But we don’t. We only know the sample mean and standard deviation, and those have some uncertainty about them. 

That uncertainty is reduced with large samples, so that the normal is “close enough.”  In small samples, the `\(t\)` distribution provides a better approximation.
]
]

![](8-sampling_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;

---
## `\(t\)` distribution

- The primary difference between the normal distribution and the `\(t\)` distribution is the fatter tails
  - This produces wider confidence intervals
  - The penalty we have to pay for our ignorance about the population

- The form of the confidence interval remains the same. We simply substitute a corresponding value from the `\(t\)` distribution (using df = `\(N -1\)`).


`$$CI_{95} = \bar{X} \pm [1.96\frac{\hat{\sigma}}{\sqrt{N}}]$$`

`$$CI_{95} = \bar{X} \pm [t_{.975, df = N-1}\frac{\hat{\sigma}}{\sqrt{N}}]$$`

---
## Confidence Intervals

What does it NOT mean?
  - there is a 95% probability that the true mean lies inside the confidence interval

--

What it *actually* means:
  - If we carried out random sampling from the population a large number of times...
  - and calculated the 95% confidence interval each time...
  - then 95% of those intervals can be expected to contain the population mean.


---

.left-column[
.small[
###Simulation

At each sample size, draw 5000 samples from known population ( `\(\mu = 0\)` , `\(\sigma = 1\)` ). 

Calculate CI for each sample using `\(s\)` and record whether or not 0 was in that interval.

Calculate CI using for each sample using `\(\sigma\)`.

]
]

![](8-sampling_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;

---

In the past, my classroom exams (aggregating over many classes) have a mean of 90 and a standard deviation of 8.

My next class will have 100 students. What range of exam means would be plausible if this class is similar to past classes (comes from the same population)?



```r
M = 90
SD = 8
N = 100

sem = SD/sqrt(N)

ci_lb_z = M - sem * qnorm(p = .975)
ci_ub_z = M + sem * qnorm(p = .975)
print(c(ci_lb_z, ci_ub_z))
```

```
## [1] 88.43203 91.56797
```

```r
ci_lb_z = M - sem * qt(p = .975, df = N-1)
ci_ub_z = M + sem * qt(p = .975, df = N-1)
print(c(ci_lb_z, ci_ub_z))
```

```
## [1] 88.41263 91.58737
```

---

I give a classroom exam that produces a mean of 83.4 and a standard deviation of 10.6. A total of 26 students took the exam.

What is the 95% confidence interval around the mean?


```r
M = 83.4
SD = 10.6
N = 26

sem = SD/sqrt(N)

ci_lb_z = M - sem * qnorm(p = .975)
ci_ub_z = M + sem * qnorm(p = .975)
print(c(ci_lb_z, ci_ub_z))
```

```
## [1] 79.32557 87.47443
```

```r
ci_lb_z = M - sem * qt(p = .975, df = N-1)
ci_ub_z = M + sem * qt(p = .975, df = N-1)
print(c(ci_lb_z, ci_ub_z))
```

```
## [1] 79.11857 87.68143
```
---


|             | Population&lt;br&gt;Distribution |  Sample&lt;br&gt;Distribution | Sampling&lt;br&gt;distribution |
|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|
| Distiribution consists of:    |    Individual observations&lt;br&gt; `\(x\)`    | Individual observations&lt;br&gt; `\(x\)`   | Statistics&lt;br&gt; `\(\bar{x}, s, s^2\)` | 
| Central tendency |    `\(\mu\)`   | `\(\bar{x}\)`      | `\(\mu_M\)` |
| Dispersion | `\(\sigma^2\)` | `\(s^2\)` | `\(\sigma^2_M\)` |
|            | `\(\sigma\)` | `\(s\)` | SEM `\(\sigma_M\)` |
| Type       | Parameter | Statistic | Statistic of statistics |
| T vs. O    | Theoretical | Observed | Theoretical

---

class:inverse

## Next time...

Exam 1 


(ahhhhhhhhhhh!)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
