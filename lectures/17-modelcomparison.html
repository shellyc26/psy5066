<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Model Comparison Approach</title>
    <meta charset="utf-8" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/rladies.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/rladies-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Model Comparison Approach

---




## Recap

* We've compared a lot of means:

  - sample mean ( `\(\bar{x}\)` ) to population mean ( `\(\mu\)` ); paired or not paired
  - 2 sample means  ( `\(\bar{x_1}\)` vs. `\(\bar{x_2}\)` )

---

## This time

* `\(t\)`-tests through oneway ANOVA
* BUT, we're going to take a different approach...

--

**Model Comparisons**

---

name: one

## Scenario 1

**Gerrymandering**
- Depending on the estimate you pick, about 53% of voters in Wisconsin were Democrats in 2016. 
- So our best estimate of the percentage of voters that are Democrat in any *district* might be 53%
- Now that 2016 feels like a million years ago, you find that in actuality it was 52% of voters in Wisconsin were Democrats in 2016.
- *Question: Was our population estimate of 53% significantly different from our sample estimate of 52%?*

--

**one-sample `\(t\)`-test**

---

## Model Comparisons

In the normal **one-sample `\(t\)`-test**

  - `\(\bar{x} = \mu\)`
  - `\(H_0: \bar{x} - \mu = 0\)`
  - `\(H_A: \bar{x} - \mu \neq 0\)`

---
## Model Comparisons

Another way of thinking about it -- what does a model of the null hypothesis look like?

--

- If there is truly no difference between means (the null is true), then the best way to summarize the data is to use the population mean. Let's use that as our estimator. 

--

- If we use the population mean as our estimator, we can look at error or *residual*. The actual data points - estimator (a deviation score!). We want to do a good job of predicting. So we want this error to be as SMALL as possible!

--

- If there *is* a difference between means (the null should be rejected), then the best way of summarizing our data should be to use the sample mean.

--

- Our goal: which had the smallest prediction error?

---

## Model Comparisons

Let's break it down into **full** and **restricted** models:

- **Restricted Model:** reflects what we are testing *against*. 

--

- **Full Model:** allows us to fully include all information we might have.

--

- Size of the effect is calculated as the following: 

`$$\frac{(E_r - E_f) / (df_r - df_f)}{E_f/df_f}$$`

where:
  - `\(E_r\)` is the prediction error from the restricted model
  - `\(E_f\)` is the prediction error from the full model
  - `\(df_r\)` is the degrees of freedom from the restricted model
  - `\(df_f\)` is the degrees of freedom from the full model

---

## The Data


```r
dems &lt;- data.frame(Dem = c(30, 69, 99, 77, 29, 37, 38, 37))
dems
```

```
##   Dem
## 1  30
## 2  69
## 3  99
## 4  77
## 5  29
## 6  37
## 7  38
## 8  37
```

---

## The Restricted Model
Step 1: Get the deviation scores. In the restricted model, we are subtracting from our population estimate of 53%. That is, 53% is our PREDICTION of the **restricted** model


```r
dems$deviationScores &lt;- dems$Dem - 53
dems
```

```
##   Dem deviationScores
## 1  30             -23
## 2  69              16
## 3  99              46
## 4  77              24
## 5  29             -24
## 6  37             -16
## 7  38             -15
## 8  37             -16
```

---
## The Restricted Model
Step 2: Square the deviation scores


```r
dems$dev2 &lt;- dems$deviationScores ^2
dems
```

```
##   Dem deviationScores dev2
## 1  30             -23  529
## 2  69              16  256
## 3  99              46 2116
## 4  77              24  576
## 5  29             -24  576
## 6  37             -16  256
## 7  38             -15  225
## 8  37             -16  256
```

---
## The Restricted Model
Step 3: Get the sum of the square deviation scores. This is our **ERROR** term. It is the squared errors. 


```r
Er &lt;- sum(dems$dev2)
dems
```

```
##   Dem deviationScores dev2
## 1  30             -23  529
## 2  69              16  256
## 3  99              46 2116
## 4  77              24  576
## 5  29             -24  576
## 6  37             -16  256
## 7  38             -15  225
## 8  37             -16  256
```

```r
Er
```

```
## [1] 4790
```

---
## The Restricted Model
Step 4: Determine the degrees of freedom. 

- Degrees of freedom deals with how much information is *free to vary*
- You get a feel for this the more you practice
- In this restricted model, there is nothing that we are "guessing" or estimating. So there is nothing to subtract. `\(df = n\)`, **df = 8**


```r
dfr &lt;- 8
```

---

## The Full Model
Step 1: Get the deviation scores. In the full model, we are subtracting from our population estimate of 52%. That is, 52% is the PREDICTION of the **full** model.


```r
demsFull = dems %&gt;% 
  select(Dem)

demsFull$deviationScores &lt;- demsFull$Dem - 52
demsFull
```

```
##   Dem deviationScores
## 1  30             -22
## 2  69              17
## 3  99              47
## 4  77              25
## 5  29             -23
## 6  37             -15
## 7  38             -14
## 8  37             -15
```

---
## The Full Model
Step 2: Square the deviation scores


```r
demsFull$dev2 &lt;- demsFull$deviationScores^2
demsFull
```

```
##   Dem deviationScores dev2
## 1  30             -22  484
## 2  69              17  289
## 3  99              47 2209
## 4  77              25  625
## 5  29             -23  529
## 6  37             -15  225
## 7  38             -14  196
## 8  37             -15  225
```

---
## The Full Model
Step 3: Get the sum of the square deviation scores. This is our **ERROR** term. It is the squared errors. 


```r
Ef &lt;- sum(demsFull$dev2)
demsFull
```

```
##   Dem deviationScores dev2
## 1  30             -22  484
## 2  69              17  289
## 3  99              47 2209
## 4  77              25  625
## 5  29             -23  529
## 6  37             -15  225
## 7  38             -14  196
## 8  37             -15  225
```

```r
Ef
```

```
## [1] 4782
```

---
## The Full Model
Step 4: Determine the degrees of freedom. 

- Degrees of freedom deals with how much information is *free to vary*
- You get a feel for this the more you practice
- In this full model, we are guessing/estimating our sample mean of 52. `\(df = n-1\)`, **df = 8 - 1 = 7**


```r
dff &lt;- 7
```

---
## The Effect
`$$\frac{(E_r - E_f) / (df_r - df_f)}{E_f/df_f}$$`


```r
effect &lt;- ((Er - Ef) / (dfr - dff)) / (Ef/dff)
effect
```

```
## [1] 0.01171058
```

--

This is our `\(F\)`-statistic. `\(t^2 = F\)`. So to get our `\(t\)`-statistic, let's take the square root of our `effect`.


```r
tstat &lt;- sqrt(effect)
round(x = tstat, digits = 3)
```

```
## [1] 0.108
```

---
## Model Comparison Approach

Is 0.108 more extreme than our critical value? 

- For an `\(\alpha = .05\)` in a two-tailed test with `\(df - 7\)`, the critical `\(t\)` value is 2.3650. 

**Conclusion: No, it's not more extreme than the critical value. The weighted error term from the restricted model is larger than the weighted error term from the full model -- 53 is a better estimator than 52. The means are not statistically significantly different**

---

## Model Comparison Approach

We just did a one-sample `\(t\)`-test! Let's verify our results:


```r
t.test(x = dems$Dem, mu = 53)
```

```
## 
## 	One Sample t-test
## 
## data:  dems$Dem
## t = -0.10822, df = 7, p-value = 0.9169
## alternative hypothesis: true mean is not equal to 53
## 95 percent confidence interval:
##  30.14892 73.85108
## sample estimates:
## mean of x 
##        52
```

---

## Look at the errors!
In general, we try to *minimize* error!

.pull-left[
Restricted

```r
as.data.frame(dems$dev2)
```

```
##   dems$dev2
## 1       529
## 2       256
## 3      2116
## 4       576
## 5       576
## 6       256
## 7       225
## 8       256
```
]

.pull-right[
Full

```r
as.data.frame(demsFull$dev2)
```

```
##   demsFull$dev2
## 1           484
## 2           289
## 3          2209
## 4           625
## 5           529
## 6           225
## 7           196
## 8           225
```

]
---
name: indep

## Scenario 2

- What if now we want to compare the difference in means (of % Democrats) between the 2010 election?
- *Question: are the means of % Democrats significantly different between 2010 and 2016?*

---

## The Data

.code-small[

```r
dems &lt;- data.frame(Dem = c(30, 69, 99, 77, 29, 37, 38, 37,
                           30, 62, 50, 69, 27, 29, 44, 45),
                   Year = c(rep("2016", times = 8),
                            rep("2010", times = 8)))
dems$Year &lt;- factor(dems$Year)

dems
```

```
##    Dem Year
## 1   30 2016
## 2   69 2016
## 3   99 2016
## 4   77 2016
## 5   29 2016
## 6   37 2016
## 7   38 2016
## 8   37 2016
## 9   30 2010
## 10  62 2010
## 11  50 2010
## 12  69 2010
## 13  27 2010
## 14  29 2010
## 15  44 2010
## 16  45 2010
```
]

---

## The Hypotheses

- `\(H_0: \bar{x}_{2010} - \bar{x}_{2016} = 0\)`
- `\(H_A: \bar{x}_{2010} - \bar{x}_{2016} \neq 0\)`

--

- Restricted Model: the best way of minimizing errors is to use the overall grand mean

- Full Model: the best way of minimizing errors is to use the group-specific mean. 

---

## The Means

Let's get the grand mean to use in our Restricted model and the means of each group (% Dem in 2010 vs. % Dem in 2016):


```r
grandMean &lt;- mean(dems$Dem)

groupMeans &lt;- dems %&gt;% 
  group_by(Year) %&gt;% 
  summarize(means = mean(Dem))

grandMean
```

```
## [1] 48.25
```

```r
groupMeans
```

```
## # A tibble: 2 x 2
##   Year  means
##   &lt;fct&gt; &lt;dbl&gt;
## 1 2010   44.5
## 2 2016   52
```

---

## The Restricted Model


```r
dems$Mean &lt;- rep(grandMean, times = nrow(dems))
dems
```

```
##    Dem Year  Mean
## 1   30 2016 48.25
## 2   69 2016 48.25
## 3   99 2016 48.25
## 4   77 2016 48.25
## 5   29 2016 48.25
## 6   37 2016 48.25
## 7   38 2016 48.25
## 8   37 2016 48.25
## 9   30 2010 48.25
## 10  62 2010 48.25
## 11  50 2010 48.25
## 12  69 2010 48.25
## 13  27 2010 48.25
## 14  29 2010 48.25
## 15  44 2010 48.25
## 16  45 2010 48.25
```

---
## The Restricted Model

Step 1: Deviation Scores


```r
dems$deviationScores &lt;- dems$Dem - dems$Mean
dems
```

```
##    Dem Year  Mean deviationScores
## 1   30 2016 48.25          -18.25
## 2   69 2016 48.25           20.75
## 3   99 2016 48.25           50.75
## 4   77 2016 48.25           28.75
## 5   29 2016 48.25          -19.25
## 6   37 2016 48.25          -11.25
## 7   38 2016 48.25          -10.25
## 8   37 2016 48.25          -11.25
## 9   30 2010 48.25          -18.25
## 10  62 2010 48.25           13.75
## 11  50 2010 48.25            1.75
## 12  69 2010 48.25           20.75
## 13  27 2010 48.25          -21.25
## 14  29 2010 48.25          -19.25
## 15  44 2010 48.25           -4.25
## 16  45 2010 48.25           -3.25
```

---
## The Restricted Model

Step 2: Square Deviation Scores


```r
dems$dev2 &lt;- dems$deviationScores ^2
dems
```

```
##    Dem Year  Mean deviationScores      dev2
## 1   30 2016 48.25          -18.25  333.0625
## 2   69 2016 48.25           20.75  430.5625
## 3   99 2016 48.25           50.75 2575.5625
## 4   77 2016 48.25           28.75  826.5625
## 5   29 2016 48.25          -19.25  370.5625
## 6   37 2016 48.25          -11.25  126.5625
## 7   38 2016 48.25          -10.25  105.0625
## 8   37 2016 48.25          -11.25  126.5625
## 9   30 2010 48.25          -18.25  333.0625
## 10  62 2010 48.25           13.75  189.0625
## 11  50 2010 48.25            1.75    3.0625
## 12  69 2010 48.25           20.75  430.5625
## 13  27 2010 48.25          -21.25  451.5625
## 14  29 2010 48.25          -19.25  370.5625
## 15  44 2010 48.25           -4.25   18.0625
## 16  45 2010 48.25           -3.25   10.5625
```

---
## The Restricted Model

Step 3: Sum of Squares -- our **ERROR** term

.pull-left[
.code-small[

```r
Er &lt;- sum(dems$dev2)
dems
```

```
##    Dem Year  Mean deviationScores      dev2
## 1   30 2016 48.25          -18.25  333.0625
## 2   69 2016 48.25           20.75  430.5625
## 3   99 2016 48.25           50.75 2575.5625
## 4   77 2016 48.25           28.75  826.5625
## 5   29 2016 48.25          -19.25  370.5625
## 6   37 2016 48.25          -11.25  126.5625
## 7   38 2016 48.25          -10.25  105.0625
## 8   37 2016 48.25          -11.25  126.5625
## 9   30 2010 48.25          -18.25  333.0625
## 10  62 2010 48.25           13.75  189.0625
## 11  50 2010 48.25            1.75    3.0625
## 12  69 2010 48.25           20.75  430.5625
## 13  27 2010 48.25          -21.25  451.5625
## 14  29 2010 48.25          -19.25  370.5625
## 15  44 2010 48.25           -4.25   18.0625
## 16  45 2010 48.25           -3.25   10.5625
```
]
]

.pull-right[
.code-small[

```r
Er
```

```
## [1] 6701
```
]
]


---
## The Restricted Model
Step 4: Determine the degrees of freedom. 

- Degrees of freedom deals with how much information is *free to vary*
- You get a feel for this the more you practice
- In this restricted model, we are guessing/estimating our grand mean of 48.25. `\(df = n-1\)`, **df = 16 - 1 = 15**


```r
dfr &lt;- 15
```

---

## The Full Model


```r
dems &lt;- data.frame(Dem = c(30, 69, 99, 77, 29, 37, 38, 37,
                           30, 62, 50, 69, 27, 29, 44, 45),
                   Year = c(rep("2016", times = 8),
                            rep("2010", times = 8)))
dems$Year &lt;- factor(dems$Year)

dems$Mean &lt;- c(rep(groupMeans$means[2], times = 8),
               rep(groupMeans$means[1], times = 8))
dems
```

```
##    Dem Year Mean
## 1   30 2016 52.0
## 2   69 2016 52.0
## 3   99 2016 52.0
## 4   77 2016 52.0
## 5   29 2016 52.0
## 6   37 2016 52.0
## 7   38 2016 52.0
## 8   37 2016 52.0
## 9   30 2010 44.5
## 10  62 2010 44.5
## 11  50 2010 44.5
## 12  69 2010 44.5
## 13  27 2010 44.5
## 14  29 2010 44.5
## 15  44 2010 44.5
## 16  45 2010 44.5
```

---

## The Full Model

Step 1: Deviation Scores


```r
dems$deviationScores &lt;- dems$Dem - dems$Mean
dems
```

```
##    Dem Year Mean deviationScores
## 1   30 2016 52.0           -22.0
## 2   69 2016 52.0            17.0
## 3   99 2016 52.0            47.0
## 4   77 2016 52.0            25.0
## 5   29 2016 52.0           -23.0
## 6   37 2016 52.0           -15.0
## 7   38 2016 52.0           -14.0
## 8   37 2016 52.0           -15.0
## 9   30 2010 44.5           -14.5
## 10  62 2010 44.5            17.5
## 11  50 2010 44.5             5.5
## 12  69 2010 44.5            24.5
## 13  27 2010 44.5           -17.5
## 14  29 2010 44.5           -15.5
## 15  44 2010 44.5            -0.5
## 16  45 2010 44.5             0.5
```

---
## The Full Model

Step 2: Square Deviation Scores


```r
dems$dev2 &lt;- dems$deviationScores ^2
dems
```

```
##    Dem Year Mean deviationScores    dev2
## 1   30 2016 52.0           -22.0  484.00
## 2   69 2016 52.0            17.0  289.00
## 3   99 2016 52.0            47.0 2209.00
## 4   77 2016 52.0            25.0  625.00
## 5   29 2016 52.0           -23.0  529.00
## 6   37 2016 52.0           -15.0  225.00
## 7   38 2016 52.0           -14.0  196.00
## 8   37 2016 52.0           -15.0  225.00
## 9   30 2010 44.5           -14.5  210.25
## 10  62 2010 44.5            17.5  306.25
## 11  50 2010 44.5             5.5   30.25
## 12  69 2010 44.5            24.5  600.25
## 13  27 2010 44.5           -17.5  306.25
## 14  29 2010 44.5           -15.5  240.25
## 15  44 2010 44.5            -0.5    0.25
## 16  45 2010 44.5             0.5    0.25
```

---
## The Full Model

Step 3: Sum of Squares -- our **ERROR** term

.pull-left[
.code-small[

```r
Ef &lt;- sum(dems$dev2)
dems
```

```
##    Dem Year Mean deviationScores    dev2
## 1   30 2016 52.0           -22.0  484.00
## 2   69 2016 52.0            17.0  289.00
## 3   99 2016 52.0            47.0 2209.00
## 4   77 2016 52.0            25.0  625.00
## 5   29 2016 52.0           -23.0  529.00
## 6   37 2016 52.0           -15.0  225.00
## 7   38 2016 52.0           -14.0  196.00
## 8   37 2016 52.0           -15.0  225.00
## 9   30 2010 44.5           -14.5  210.25
## 10  62 2010 44.5            17.5  306.25
## 11  50 2010 44.5             5.5   30.25
## 12  69 2010 44.5            24.5  600.25
## 13  27 2010 44.5           -17.5  306.25
## 14  29 2010 44.5           -15.5  240.25
## 15  44 2010 44.5            -0.5    0.25
## 16  45 2010 44.5             0.5    0.25
```
]
]

.pull-right[
.code-small[

```r
Ef
```

```
## [1] 6476
```
]
]

---
## The Full Model
Step 4: Determine the degrees of freedom. 

- Degrees of freedom deals with how much information is *free to vary*
- You get a feel for this the more you practice
- In this full model, we are guessing/estimating our 2 means (mean for 2010 and mean for 2016). `\(df = n-2\)`, **df = 16 - 2 = 14**


```r
dff &lt;- 14
```

---
## The Effect
`$$\frac{(E_r - E_f) / (df_r - df_f)}{E_f/df_f}$$`


```r
effect &lt;- ((Er - Ef) / (dfr - dff)) / (Ef/dff)
effect
```

```
## [1] 0.4864114
```

--

This is our `\(F\)`-statistic. Remember that `\(t^2 = F\)`. So to get our `\(t\)`-statistic, let's take the square root of our `effect`.


```r
tstat &lt;- sqrt(effect)
round(x = tstat, digits = 3)
```

```
## [1] 0.697
```

---
## Model Comparison Approach

Is 0.697 more extreme than our critical value? 

- For an `\(\alpha = .05\)` in a two-tailed test with `\(df = 14\)`, the critical `\(t\)` value is 2.145. 

**Conclusion: No, it's not more extreme than the critical value. The weighted error term for the restricted is larger than the weighted error term from the full. The grand mean was a better estimator than using individual group means. Therefore, the means are not statistically significantly different.**

---

## Model Comparison Approach

We just did an independent-samples `\(t\)`-test! Let's verify our results:


```r
t.test(dems$Dem ~ dems$Year)
```

```
## 
## 	Welch Two Sample t-test
## 
## data:  dems$Dem by dems$Year
## t = -0.69743, df = 11.406, p-value = 0.4995
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -31.06632  16.06632
## sample estimates:
## mean in group 2010 mean in group 2016 
##               44.5               52.0
```

---
name: aov

## Scenario 3

- We have a dataset that looks at the lengths and widths of petals &amp; sepals of the iris flower. It includes 3 different species of irises. 
- *Question: are the sepal lengths different amongst the 3 species of irises?*

---

## The Data


```r
head(iris)
```

```
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa
```

```r
iris &lt;- iris %&gt;% 
  select(Sepal.Length, Species)
```

---

## The Hypotheses

- `\(H_0: \bar{x}_{setosa} = \bar{x}_{versicolor} = \bar{x}_{virginica}\)`
- `\(H_A: \bar{x}_{setosa} \neq \bar{x}_{versicolor} \neq \bar{x}_{virginica}\)`

--

- Restricted Model: the best way of minimizing errors is to use the overall grand mean
- Full Model: the best way of minimizing errors is to use the group-specific means 

---

## The Means

Let's get the grand mean to use in our Restricted model and the means of each group:


```r
grandMean &lt;- mean(iris$Sepal.Length)

groupMeans &lt;- iris %&gt;% 
  group_by(Species) %&gt;% 
  summarize(means = mean(Sepal.Length))

grandMean
```

```
## [1] 5.843333
```

```r
groupMeans
```

```
## # A tibble: 3 x 2
##   Species    means
##   &lt;fct&gt;      &lt;dbl&gt;
## 1 setosa      5.01
## 2 versicolor  5.94
## 3 virginica   6.59
```

---

## The Restricted Model


```r
restricted &lt;- iris
restricted$Mean &lt;- rep(grandMean, times = nrow(restricted))
head(restricted)
```

```
##   Sepal.Length Species     Mean
## 1          5.1  setosa 5.843333
## 2          4.9  setosa 5.843333
## 3          4.7  setosa 5.843333
## 4          4.6  setosa 5.843333
## 5          5.0  setosa 5.843333
## 6          5.4  setosa 5.843333
```

---
## The Restricted Model

Step 1: Deviation Scores


```r
restricted$deviationScores &lt;- restricted$Sepal.Length - restricted$Mean
head(restricted)
```

```
##   Sepal.Length Species     Mean deviationScores
## 1          5.1  setosa 5.843333      -0.7433333
## 2          4.9  setosa 5.843333      -0.9433333
## 3          4.7  setosa 5.843333      -1.1433333
## 4          4.6  setosa 5.843333      -1.2433333
## 5          5.0  setosa 5.843333      -0.8433333
## 6          5.4  setosa 5.843333      -0.4433333
```

---
## The Restricted Model

Step 2: Square Deviation Scores


```r
restricted$dev2 &lt;- restricted$deviationScores ^2
head(restricted)
```

```
##   Sepal.Length Species     Mean deviationScores      dev2
## 1          5.1  setosa 5.843333      -0.7433333 0.5525444
## 2          4.9  setosa 5.843333      -0.9433333 0.8898778
## 3          4.7  setosa 5.843333      -1.1433333 1.3072111
## 4          4.6  setosa 5.843333      -1.2433333 1.5458778
## 5          5.0  setosa 5.843333      -0.8433333 0.7112111
## 6          5.4  setosa 5.843333      -0.4433333 0.1965444
```

---
## The Restricted Model

Step 3: Sum of Squares -- our **ERROR** term


```r
Er &lt;- sum(restricted$dev2)
head(restricted)
```

```
##   Sepal.Length Species     Mean deviationScores      dev2
## 1          5.1  setosa 5.843333      -0.7433333 0.5525444
## 2          4.9  setosa 5.843333      -0.9433333 0.8898778
## 3          4.7  setosa 5.843333      -1.1433333 1.3072111
## 4          4.6  setosa 5.843333      -1.2433333 1.5458778
## 5          5.0  setosa 5.843333      -0.8433333 0.7112111
## 6          5.4  setosa 5.843333      -0.4433333 0.1965444
```

```r
Er
```

```
## [1] 102.1683
```

---
## The Restricted Model
Step 4: Determine the degrees of freedom. 

- Degrees of freedom deals with how much information is *free to vary*
- You get a feel for this the more you practice
- In this restricted model, we are guessing/estimating our grand mean of 5.843. `\(df = n-1\)`, **df = 150 - 1 = 149**


```r
dfr &lt;- 149
```

---

## The Full Model


```r
full &lt;- iris
full$Mean &lt;- c(rep(groupMeans$means[1], times = 50),
               rep(groupMeans$means[2], times = 50),
               rep(groupMeans$means[3], times = 50))
head(full)
```

```
##   Sepal.Length Species  Mean
## 1          5.1  setosa 5.006
## 2          4.9  setosa 5.006
## 3          4.7  setosa 5.006
## 4          4.6  setosa 5.006
## 5          5.0  setosa 5.006
## 6          5.4  setosa 5.006
```

---

## The Full Model

Step 1: Deviation Scores


```r
full$deviationScores &lt;- full$Sepal.Length - full$Mean
head(full)
```

```
##   Sepal.Length Species  Mean deviationScores
## 1          5.1  setosa 5.006           0.094
## 2          4.9  setosa 5.006          -0.106
## 3          4.7  setosa 5.006          -0.306
## 4          4.6  setosa 5.006          -0.406
## 5          5.0  setosa 5.006          -0.006
## 6          5.4  setosa 5.006           0.394
```

---
## The Full Model

Step 2: Square Deviation Scores


```r
full$dev2 &lt;- full$deviationScores ^2
head(full)
```

```
##   Sepal.Length Species  Mean deviationScores     dev2
## 1          5.1  setosa 5.006           0.094 0.008836
## 2          4.9  setosa 5.006          -0.106 0.011236
## 3          4.7  setosa 5.006          -0.306 0.093636
## 4          4.6  setosa 5.006          -0.406 0.164836
## 5          5.0  setosa 5.006          -0.006 0.000036
## 6          5.4  setosa 5.006           0.394 0.155236
```

---
## The Full Model

Step 3: Sum of Squares -- our **ERROR** term


```r
Ef &lt;- sum(full$dev2)
head(full)
```

```
##   Sepal.Length Species  Mean deviationScores     dev2
## 1          5.1  setosa 5.006           0.094 0.008836
## 2          4.9  setosa 5.006          -0.106 0.011236
## 3          4.7  setosa 5.006          -0.306 0.093636
## 4          4.6  setosa 5.006          -0.406 0.164836
## 5          5.0  setosa 5.006          -0.006 0.000036
## 6          5.4  setosa 5.006           0.394 0.155236
```

```r
Ef
```

```
## [1] 38.9562
```

---
## The Full Model
Step 4: Determine the degrees of freedom. 

- Degrees of freedom deals with how much information is *free to vary*
- You get a feel for this the more you practice
- In this full model, we are guessing/estimating our 3 means (mean for each species). `\(df = n-3\)`, **df = 150 - 3 = 147**


```r
dff &lt;- 147
```

---
## The Effect
`$$\frac{(E_r - E_f) / (df_r - df_f)}{E_f/df_f}$$`


```r
effect &lt;- ((Er - Ef) / (dfr - dff)) / (Ef/dff)
effect
```

```
## [1] 119.2645
```

--

This is our `\(F\)`-statistic. This is an ANOVA, so we can stick with the `\(F\)`-statistic. (back to this in a sec)


```r
round(x = effect, digits = 3)
```

```
## [1] 119.265
```

---
## Model Comparison Approach

Is 119.265 more extreme than our critical value? 

- A significant `\(F\)`-statistic is anything above 1. Yes, our value is larger than 1.

**Conclusion: The weighted error term of the restricted is smaller than the weighted error term of the full. Using each group's mean was a better estimator, compared to the overall grand mean. Therefore, the means are statistically significantly different.**

---

## Model Comparison Approach

We just did a oneway ANOVA! Let's verify our results:


```r
summary(aov(Sepal.Length ~ Species, data = iris))
```

```
##              Df Sum Sq Mean Sq F value Pr(&gt;F)    
## Species       2  63.21  31.606   119.3 &lt;2e-16 ***
## Residuals   147  38.96   0.265                   
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

---

## Linking Distributions

- ANOVA is a comparison of means. But we are interested in variance. That is, we are trying to figure out the source of the variance (more on this next time)
- When we want to make inferences about sample variability, we need to know the sampling distribution.
- We can make use of the `\(\chi^2\)` distribution here. It has a single parameter, `\(\nu\)`, related to the sample size, `\(N\)`. There is an important relationship between the normal distribution and the `\(\chi^2\)` distribution:
  - The sum of squared standard normal variables will have a `\(\chi^2\)` distribution. 

---
## Linking Distributions

- The `\(F\)`-distribution is a ratio of two `\(\chi^2\)` variables -- 2 different variances
- However, it's weighted by degrees of freedom. But there needs to be a df for the numerator AND df for the denominator
- So the `\(F\)`-distribution has 2 parameters: 2 different degrees of freedom. As we've talked about it thus far, think of this as the df for the full and restricted models. Next time, we'll translate into ANOVA terminology

---
`$$F_{\nu_1\nu_2} = \frac{\frac{\chi^2_{\nu_1}}{\nu_1}}{\frac{\chi^2_{\nu_2}}{\nu_2}}$$`

The `\(F\)`-distribution is one-sided (like the `\(\chi^2\)` ). You only care about the upper tail. You can't have a negative variance. If this is the ratio of 2 variances (ish), it's not going to be negative ( `\(F\)`-statistic `\(\geq 0\)` )

---

&lt;center&gt;

&lt;img src="17-modelcomparison_files/figure-html/fdist.png", width="50%"&gt;

&lt;/center&gt;

The `\(t\)`, `\(\chi^2\)`, and `\(F\)` distributions all depend on the normal distribution assumption. The normal distribution is the "parent" population.

As sample size increases, `\(t\)` and `\(\chi^2\)` converge on the normal. `\(F\)` converges on :

`$$\frac{\chi^2_{\nu_1}}{\nu_1}$$`
with the numerator depending on the normal.

---

## But keep in mind

These probability distributions have a key assumption:

  - the sample is a random selection from the population
    

If the sample is not a random selection, the rules of probability don't apply.

---

## Utility

We have programs like `R`. In the workforce, no one will expect you to calculate this stuff by hand. So why go through the effort of showing you this?

--

A model is what **YOU** define. It's how you think the world works. The restricted model is really just an emobodiement of the null hypothesis! The full model is the embodiment of the alternative hypothesis!

--

Minimizing error terms is how we evaluate multitudes of models!

--

Plus, model comparison frameworks come up more formally in some advanced types of statistics. 

---

class: inverse

## Next time

Translate all of this into classic, textbook ANOVA terminology
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
