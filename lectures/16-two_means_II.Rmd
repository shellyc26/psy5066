---
title: 'Comparing two means II'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, rladies, rladies-fonts, "my-theme.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(here)
library(knitr)
library(kableExtra)
library(ggpubr)
```

## Reminders

Quiz 9 is on Thursday next week

Sign up for your oral exam time slot ASAP!

---

## Last time

* Introduction to independent samples *t*-tests

  * Standard error of the **difference of means**
  
  * Pooled variance, $\hat{\sigma}_P$
  
  * Confidence intervals around the difference in means
  
  * Confidence intervals around different means

---

## Review of independent samples *t*-tests

It's generally argued that Republicans have an age problem -- but are they substantially older than Democrats? 

In 2014 (midterm election before the most recent presidential election), [*Five Thirty Eight* did an analysis](https://fivethirtyeight.com/features/both-republicans-and-democrats-have-an-age-problem/) of the ages of elected members of Congress. They've provided their data, so we can run analyses on our own.

```{r, echo = 1, message = F}
library(fivethirtyeight)
library(tidyverse)
```
---

```{r}
congress = congress_age %>%
  filter(congress == 113) %>% # just the most recent
  filter(party %in% c("R", "D")) # remove independents
```

```{r}
psych::describe(congress$age)
```
---

```{r}
congress %>% ggplot(aes(x = age)) + geom_histogram(bins = 50, color = "white") + labs(x = "Age in years", y = "Frequency", title = "Age Distribution of Congressional \nMembers in 113th Congress", caption = "This is pretty normal.") + theme_bw(base_size = 20)
```
---

```{r}
congress %>% ggplot(aes(x = age, fill = party)) + geom_histogram(bins = 50, color = "white", position = "dodge") + labs(x = "Age in years", y = "Frequency", title = "Age Distribution of Congressional \nMembers in 113th Congress", caption = "This is pretty normal.") + scale_fill_manual(values = c("blue", "red")) + theme_bw(base_size = 20)
```

---

```{r}
psych::describeBy(congress$age, group = congress$party)
```


```{r, echo = F}
stats = psych::describeBy(congress$age, group = congress$party)
```

.pull-left[
$$
\begin{aligned}
 \bar{X}_1 &= `r round(stats$D[1, "mean"],2)` \\
 \hat{\sigma}_1 &= `r round(stats$D[1, "sd"],2)` \\
 N_1 &= `r round(stats$D[1, "n"])` \\
\end{aligned}
$$
]

.pull-right[
$$
\begin{aligned}
 \bar{X}_2 &= `r round(stats$R[1, "mean"],2)` \\
 \hat{\sigma}_2 &= `r round(stats$R[1, "sd"],2)` \\
 N_2 &= `r round(stats$R[1, "n"])` \\
\end{aligned}
$$
]

---

```{r, echo = F}
x1 = stats$D[1, "mean"]
s1 = stats$D[1, "sd"]
n1 = stats$D[1, "n"]

x2 = stats$R[1, "mean"]
s2 = stats$R[1, "sd"]
n2 = stats$R[1, "n"]

rx1 = round(x1, 2)
rs1 = round(s1, 2)
rn1 = round(n1, 2)
rx2 = round(x2, 2)
rs2 = round(s2, 2)
rn2 = round(n2, 2)

s=sqrt(((n1-1)*s1^2+(n2-1)*s2^2)/(n1+n2-2))

s_d = s*sqrt((1/n1) + (1/n2))

df = n1+n2-2
```


Next we build the sampling distribution under the null hypotheses.

$$
\begin{aligned}
 \mu &= 0 \\
 \\
 \sigma_D &= \sqrt{\frac{(`r rn1`-1){`r rs1`}^2 + (`r rn2`-1){`r rs2`}^2}{`r rn1`+`r rn2`-2}} \sqrt{\frac{1}{`r rn1`} + \frac{1}{`r rn2`}}\\
 &= `r round(s,2)`\sqrt{\frac{1}{`r rn1`} + \frac{1}{`r rn2`}} = `r round(s_d,2)`\\
 \\
 df &= `r n1+n2-2`
\end{aligned}
$$ 
---

```{r, echo = 2}
(cv = qt(p = .975, df = 540))
(t = (x1-x2)/s_d)

x = c(-5:5)

data.frame(x) %>%
  ggplot(aes(x=x)) +
  stat_function(fun= function(x) dt(x, df = df), geom = "area", 
                xlim = c(cv, 5), fill = "purple") +
  stat_function(fun= function(x) dt(x, df = df), geom = "area", 
                xlim = c(-5, cv*-1), fill = "purple") +
  stat_function(fun= function(x) dt(x, df = df), geom = "line") +
  geom_vline(aes(xintercept = t), color = "black")+
  labs(x = "Difference in means", y = "density") +
  theme_light(base_size = 20)

```

---

```{r}
library(ggpubr)
test = list(c("R", "D"))
ggerrorplot(congress, x = "party", y = "age", desc_stat = "mean_ci", xlab = "Party", ylab = "Average Age", 
            color = "party", palette = c("red", "blue"))
```

---

```{r}
test = list(c("R", "D"))
ggerrorplot(congress, x = "party", y = "age", desc_stat = "mean_ci", xlab = "Party", ylab = "Average Age", 
            color = "party", palette = c("red", "blue"), add = "violin") +
  stat_compare_means(comparisons = test)
```

---

### A difference in significance is not a significant difference

Researchers commonly examine effects within separate groups. This can be an appropriate strategy, especially if there is reason to think that the mechanisms linking X to Y differ by groups. 

For example, traditional benchmarks might suggest that the average young adult reports a stress-level of 3.4 out of 5. But these benchmarks are estimated from a population of white young adults; you conduct a study looking at young Hispanic adults and young African American adults. 

```{r, echo = F}
calc_m_sd = function(n, lower, upper){
  mean = (upper+lower)/2
  cv = qt(p = .975, df = n-1)
  moe = upper-mean
  sem = moe/cv
  sd = sem*sqrt(n)
  return(list(mean = mean, 
              sd = sd, 
              sem = sem))
}

hispanic = calc_m_sd(n = 36, lower = 3.3, upper = 3.8)
black = calc_m_sd(n = 62, lower = 3.5, upper = 3.9)
```


.pull-left[
$$\bar{X}_H: `r hispanic$mean`$$
$$s_H: `r round(hispanic$sd,3)`$$
$$N_H: 36$$
]

.pull-left[
$$\bar{X}_B: `r black$mean`$$
$$s_B: `r round(black$sd,3)`$$
$$N_B: 62$$
]

---

We can plot these effects against our population mean:

```{r, echo = F}
data.frame(group = c("hispanic", "black"), 
           mean = c(hispanic$mean, black$mean),
           sem = c(hispanic$sem, black$sem),
           n = c(36, 62)
           ) %>%
  mutate(cv = qt(df = n-1, p = .975),
         moe = cv*sem) %>%
  ggplot(aes(x = group, y = mean, color = group)) +
  geom_point(stat = "identity", size = 3) +
  geom_errorbar(aes(ymin = mean-moe, ymax = mean+moe), size = 1.5) +
  geom_hline(aes(yintercept = 3.4), color = "black", linetype = "dashed", size = 1.5) +
  labs(x = "Group", y = "Average Stress Level", title = "Average stress level of ethnic groups", caption = "These groups are not significantly different.\nAfrican Americans are sig. different from whites,\nbut Hispanics are not.")+
  theme_bw(base_size = 20)

```

???

These groups are not significantly different from each other. Just because one group is sig and the other is not, doesn't mean they are different.

This expands beyond one-sample t-tests. For example, if there is a sig. correlation in one group but not another, or a sig ANOVA, or a sig whatever; you have to formally test to say two groups are different.

---

## Effect sizes

Cohen suggested one of the most common effect size estimates—the standardized mean difference—useful when comparing a group mean to a population mean or two group means to each other. This is referred to as **Cohen's d**. 

$$\delta = \frac{\mu_1 - \mu_0}{\sigma} \approx d = \frac{\bar{X}_1-\bar{X}_2}{\hat{\sigma}_p}$$

--

Cohen’s d is in the standard deviation (Z) metric.


What happens to Cohen's d as sample size gets larger?



---

Let's go back to our politics example:

.pull-left[
**Democrats**
$$
\begin{aligned}
 \bar{X}_1 &= `r round(stats$D[1, "mean"],2)` \\
 \hat{\sigma}_1 &= `r round(stats$D[1, "sd"],2)` \\
 N_1 &= `r round(stats$D[1, "n"])` \\
\end{aligned}
$$
]

.pull-right[
**Republicans**
$$
\begin{aligned}
 \bar{X}_2 &= `r round(stats$R[1, "mean"],2)` \\
 \hat{\sigma}_2 &= `r round(stats$R[1, "sd"],2)` \\
 N_2 &= `r round(stats$R[1, "n"])` \\
\end{aligned}
$$
]

$$\hat{\sigma}_p = \sqrt{\frac{(`r rn1`-1){`r rs1`}^2 + (`r rn2`-1){`r rs2`}^2}{`r rn1`+`r rn2`-2}} 
 = `r round(s,2)`$$
 
```{r, echo = F}
d = (x1-x2)/s
```
 
 
$$d = \frac{`r round(x1,2)`-`r round(x2,2)`}{`r round(s,2)`} = `r round(d,2)`$$

How do we interpret this? Is this a large effect?

---

Cohen (1988) suggests the following guidelines for interpreting the size of d:

.large[

- 	.2 = Small

- 	.5 = Medium

- 	.8 = Large	

]

An aside, to calculate Cohen's D for a one-sample *t*-test: 
$$d = \frac{\bar{X}-\mu}{\hat{\sigma}}$$


.small[Cohen, J. (1988), Statistical power analysis for the behavioral sciences (2nd Ed.). Hillsdale: Lawrence Erlbaum.]

---

Another useful metric is the overlap between the two population distributions -- the smaller the overlap, the farther apart the distributions. As a reminder, our data constitutes only samples representing the larger populations, so we use our statistics to build estimated population distributions.

---

```{r overlap_plot, echo = F, message=F, warning = F}
x = seq(from = 20, to = 100, by = 1)
d.alt = dnorm(x, mean = x1, sd = s)
d.nul = dnorm(x, mean = x2, sd = s)

loc = min(which(d.alt-d.nul > 0))
loc = (x[loc] + x[loc-1])/2

area = pnorm(loc, mean = x2, sd = s, lower.tail = F) + pnorm(loc, mean = x1, sd = s, lower.tail = T)

p = ggplot(data.frame(x=x), aes(x)) +
  stat_function(fun = function(x) dnorm(x, mean = x2, sd = s), 
                geom = "line", aes(color = "Republicans")) + 
  stat_function(fun = function(x) dnorm(x, mean = x1, sd = s), 
                geom = "line", aes(color = "Democrats")) + 
  scale_color_discrete("Model") +
  ggtitle(paste0("Overlap = ", round(area,3))) + 
  scale_color_manual(values = c("blue", "red")) +
  labs(x = "Congressperson Age",
          y = "Density", 
          title = "Estimated population distributions of\ncongresspersons by party")+
  theme_bw(base_size = 20)

p
```


---

```{r, echo = F}
p +   
  stat_function(fun = function(x) dnorm(x, mean = x2, sd = s),
                geom = "area", xlim = c(loc,100), fill = "black") +
  stat_function(fun = function(x) dnorm(x, mean = x1, sd = s),
                geom = "area", xlim =c(20, loc), fill = "black")

```


These distributions have `r round(area*100,2)` % overlap.

---
### Calculation of distribution overlap

There is a straightforward relationship between Cohen's d and distribution overlap:

$$\text{Overlap} = P(X \leq -\frac{|d|}{2})*2$$

```{r}
2 * pnorm(-abs(0.35)/2)
```

---

## When should you not use Student's *t*-test?

Recall the three assumptions of Student's *t*-test:

* Independence
* Normality
* Homogeneity of variance

There are no good statistical tests to determine whether you've violated the first -- it depends on how you sampled your population. 

--

* Draw phone numbers at random from a phone book?
* Recruit random sets of fraternal twins?
* Randomly select houses in a city and interview the person who answers the door?

---

### Homogeneity of variance

Homogeneity of variance can be checked with Levene’s procedure.  It tests the null hypothesis that the variances for two or more groups are equal (or within sampling variability of each other): 

$$H_0: \sigma^2_1 = \sigma^2_2$$
$$H_0: \sigma^2_1 \neq \sigma^2_2$$
Levene's test can be expanded to more than two variances; in this case, the alternative hypothesis is that at least one variance is different from at least one other variance. 

Levene's produces a statistic, *W*, that is *F* distributed with degrees of freedom of $k-1$ and $N-k$.

---

```{r, warning = F, message=F}
library(car)
leveneTest(age~party, data = congress, center = "mean")
```


Like other tests of significance, Levene’s test gets more powerful as sample size increases. So unless your two variances are *exactly* equal to each other (and if they are, you don't need to do a test), your test will be "significant" with a large enough sample. Part of the analysis has to be an eyeball test -- is this "significant" because they are truly different, or because I have many subjects. 

---
### Homogeneity of variance

The homogeneity assumption is often the hardest to justify, empirically or conceptually.

If we suspect the means for the two groups could be different (H1), that might extend to their variances as well.

Treatments that alter the means for the groups could also alter the variances for the groups.

Welch’s *t*-test removes the homogeneity requirement, but uses a different calculation for the standard error of the mean difference and the degrees of freedom.  One way to think about the Welch test is that it is a penalized *t*-test, with the penalty imposed on the degrees of freedom in relation to violation of variance homogeneity (and differences in sample size).

---

### Welch's *t*-test

$$t = \frac{\bar{X}_1-\bar{X_2}}{\sqrt{\frac{\hat{\sigma}^2_1}{N_1}+\frac{\hat{\sigma}^2_2}{N_2}}}$$

So that's a bit different -- the main difference here is the way that we weight sample variances. It's true that larger samples still get more weight, but not as much as in Student's version. Also, we divide variances here by N instead of N-1, making our standard error larger. 

???

### Don't go into the weighting of variances thing -- too confusing

---

### Welch's *t*-test

The degrees of freedom are different: 

$$df = \frac{[\frac{\hat{\sigma}^2_1}{N_1}+\frac{\hat{\sigma}^2_2}{N_2}]^2}{\frac{[\frac{\hat{\sigma}^2_1}{N1}]^2}{N_1-1}+\frac{[\frac{\hat{\sigma}^2_2}{N2}]^2}{N_2-1}}$$

These degrees of freedom can be fractional.  As the sample variances converge to equality, these df approach those for Student’s *t*, for equal N.  

---

```{r}
t.test(age~party, data = congress, var.equal = F)
```


---

### Normality

Finally, there's the assumption of normality. Specifically, this is the assumption that the population is normal -- if the population is normal, then our sampling distribution is **definitely** normal and we can use a *t*-distribution.

But even if the population is not normal, the CLT lets us assume our sampling distribution is normal because as N approaches infinity, the sampling distributions approaches normality. So we can be **pretty sure** the sampling distribution is normal. 

One thing we can check -- the only distribution we actually have access to -- is the sample distribution. If this is normal, then we can again be pretty sure that our population distribution is normal, and thus our sampling distribution is normal too. 

---

Normality can be checked with a formal test: the Shapiro-Wilk test.  The test statistic, W, has an expected value of 1 under the null hypothesis. Departures from normality reduce the size of W.  A statistically significant W is a signal that the sample distribution departs significantly from normal.

But...
* With large samples, even trivial departures will be statistically significant.
* With large samples, the sampling distribution of the mean(s) will approach normality, unless the data are very non-normally distributed.
* Visual inspection of the data can confirm if the latter is a problem.
* Visual inspection can also identify outliers that might influence the data.

---

```{r}
shapiro.test(x = congress$age)
```

.pull-left[
```{r, mwarning = F, message = F}
hist(congress$age, col = "purple", border = "white")
```
]

---

A common non-parametric test that can be used in place of the independent samples *t*-test is the **Wilcoxon sum rank test**.  Here's the calculation:

* Order all the data points by their outcome. 
* For one of the groups, add up all the ranks. That's your test statistic, *W*.
* To build the sampling distribution, randomly shuffle the group labels and add up the ranks for your group of interest again. Repeat this process until you've calculated the rank sum for every possible group assignment. 

```{r}
wilcox.test(age~party, data = congress)
```


---

## Matrix algebra

For now, we're going to focus on how we can use matrix algebra to represent the underlying models of these tests, but we're going to ignore the probability/significance part. 

At it's heart, the independent samples *t*-test proposes a model in which our best guess of a person's score is the mean of their group. 

We can use **linear combinations** of both rows and columns to generate these "best guess" scores. 

---
Assume your data take the following form:

.pull-left[

$$\large \mathbf{X} = \left(\begin{array}
{rr}
\text{Party} & \text{Age} \\
D & 85.9  \\
R & 78.8  \\
\vdots & \vdots  \\
D & 83.2 
\end{array}\right)$$
]

.pull-right[
What are your linear combinations of columns?


What are your linear combinations of rows?
]

--

**Linear combinations of columns:** 

$$\large \mathbf{T_C} = \left(\begin{array}
{rr}
0, 1
\end{array}\right)$$

---
Assume your data take the following form:

.pull-left[

$$\large \mathbf{X} = \left(\begin{array}
{rr}
\text{Party} & \text{Age} \\
D & 85.9  \\
R & 78.8  \\
\vdots & \vdots  \\
D & 83.2 
\end{array}\right)$$
]

.pull-right[
What are your linear combinations of columns?


What are your linear combinations of rows?
]



**Linear combinations of rows:** 

$$\mathbf{T_R} = \left(\begin{array}
{rr}
\frac{1}{N_D} & 0 \\
0 & \frac{1}{N_R} \\
\vdots & \vdots  \\
\frac{1}{N_D} & 0 \\
\end{array}\right)$$

---

$$(T_R)X(T_C) = M$$

This set of linear combinations calculates the different means of our two groups. That's great! 


But there's another way. We can create a single transformation matrix, T, that's very simple:

$$\mathbf{T} = \left(\begin{array}
{rr}
1 & 0 \\
1 & 1 \\
\vdots & \vdots  \\
1 & 0 \\
\end{array}\right)$$

Using this matrix, we can solve a different equation:

$$(T'T)^{-1}T'X = (b)$$
In this case, *b* will be a vector with 2 scalars.

---

```{r, echo = 1:3}
X = congress$age
T_mat = matrix(1, ncol = 2, nrow = length(X))
T_mat[congress$party == "D", 2] = 0
head(T_mat)
```

$$(T'T)^{-1}T'X = (b)$$

```{r}
solve(t(T_mat) %*% T_mat) %*% t(T_mat) %*% X
```

What do these numbers represent?

---

class: inverse

## Next time...

paired-samples *t*-tests




