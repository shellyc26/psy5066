---
title: "Comparing Means"
output:
  xaringan::moon_reader:
    includes: 
    lib_dir: libs
    css: [default, rladies, rladies-fonts, "my-theme.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide'}
library(tidyverse)
library(psych)
```
Borrowing and stealing heavily from Cooper (2020)

**Hopefully without things cut off on the bottom!**
---
## Independent samples *t*-test

- Independent samples *t*-tests are used when you want to compare means from two independent samples

- Samples are independent if observations from one sample do not affect or depend on observations from the other sample. A score from sample 1 shouldn't tell me anything about a score from sample 2 (e.g., whether the score is above or below the sample mean) 

- Basically, you want independence of observations within your samples and between your samples for this analysis

- Very common with experimental research (e.g., compare Treatment condition mean with Control condition mean)
---
## Hypotheses 
$$H_0: \mu_1 = \mu_2$$
$$H_1: \mu_1 \neq \mu_2$$
Alternatively. . . (rearranged formula)
$$H_0: \mu_1 - \mu_2 = 0$$
$$H_1: \mu_1 - \mu_2 \neq\  0$$
---
We can calculate the test statistic with:

$$t_{df} = \frac{\bar{X}_1 - \bar{X}_2}{SE_{meandifference}}$$
Technically, the formula is:
$$t_{df} = \frac{(\bar{X}_1 - \bar{X}_2)-(\mu_1 - \mu_2)}{SE_{meandifference}}$$
- $(\bar{X}_1 - \bar{X}_2)$ is our observed difference in (independent) means
- Plug in values for $(\mu_1 - \mu_2)$ based on our null (usually that $\mu_1 - \mu_2$ = 0 but doesn't have to be)

---
We assume homogeneity of variance for independent samples *t*-tests (i.e., even if samples come from different populations, these populations differ in mean but not variance), but in practice, this is unrealistic. We will most likely have two *different* variances

Standard error is easy to calculate when you only know one *SD* (e.g., one-sample *t*-tests). But we have two *SD*s, so unless they're the exact same, "we" need to calculate a weighted combination of the two.
---
Two main methods of calculating the *SE*:
1. Student's *t*-test is used when the variances of each sample are (roughly) the same. In many cases, slightly more powerful test but the **pooled variance** will be biased toward the sample with the larger sample size.
2. Welch's *t*-test if the variances are NOT (roughly) the same. Slightly less powerful.
---
**Student's assumptions:**
- Independence 
- Normality of *Population* that each sample comes from
- Homogeneity of variance for *Population*

**Welch's assumptions:**
- Independence
- Normality of *Population* that each sample comes from

---
## Variance Calculations
Welch's
- Calculate variance separately for each sample

Student's
$$\large{\hat{\sigma}^2_p = \frac{(N_1-1)\hat{\sigma}^2_1 + (N_2-1)\hat{\sigma}^2_2}{N_1+N_2-2}}$$
---
## *SD* Calculations
**Welch's**
- Calculate SD separately for each sample

**Student's**
$$\sqrt{\frac{(N_1-1)\hat{\sigma}^2_1 + (N_2-1)\hat{\sigma}^2_2}{N_1+N_2-2}} $$
---
## $SE_{diff}$  Calculations
$SE_{diff} = \hat{\sigma}_D$

**Welch's **
$$\hat{\sigma}_D = \sqrt{\frac{\hat{\sigma}^2_1}{N_1}+\frac{\hat{\sigma}^2_2}{N_2}}$$
Essentially, the squared $SE_{mean}$ of each sample is added then you take the square root.

**Student's**
$$\hat{\sigma}_D = \sqrt{\frac{(N_1-1)\hat{\sigma}^2_1 + (N_2-1)\hat{\sigma}^2_2}{N_1+N_2-2}} \sqrt{\frac{1}{N_1} + \frac{1}{N_2}}$$
---

Final result is *SE* of the difference of independent means. Remember that standard deviations estimate population variability (i.e., variability of population scores). Taking the next step and dividing the *SD* by the square root of some variation of N (depending on the test equation) helps us estimate variability of means rather than scores. 
---
**Welch's **
$$\large{\hat{\sigma}_D = \sqrt{\frac{\hat{\sigma}^2_1}{N_1}+\frac{\hat{\sigma}^2_2}{N_2}}}$$

**Student's**
$$\large{\hat{\sigma}_D = \sqrt{\frac{(N_1-1)\hat{\sigma}^2_1 + (N_2-1)\hat{\sigma}^2_2}{N_1+N_2-2}} \sqrt{\frac{1}{N_1} + \frac{1}{N_2}}}$$
Note that in Student's calculation, the variance for the larger sample will be weighted more heavily because each individual variance is being multiplied by N-1. Meanwhile, in Welch's, the standard deviations are being *divided* by sample size, so smaller samples will be weighted more heavily. 

---
$$\large{\hat{\sigma}_D = \sqrt{\frac{\hat{\sigma}^2_1}{N_1}+\frac{\hat{\sigma}^2_2}{N_2}}}$$
```{r}
# Welch's SE: sqrt(SD1^2/n1 + SD2^2/n2)
# random values
sqrt(4^2/1000 + 7^2/30)
sqrt(7^2/30)
```
One large sample can't make up for a small sample with Welch's. 

---
## *df*
Welch's
$$df = \frac{[\frac{\hat{\sigma}^2_1}{N_1}+\frac{\hat{\sigma}^2_2}{N_2}]^2}{\frac{[\frac{\hat{\sigma}^2_1}{N1}]^2}{N_1-1}+\frac{[\frac{\hat{\sigma}^2_2}{N2}]^2}{N_2-1}}$$
Welch's *df*  will likely not be a whole number. And while Welch's allows for violations of homogeneity (i.e., unequal variance), you'll still be punished for it with reduced *df* unless you have equal variance and equal sample size. This doesn't mean that Welch can't have a lower *p*-value than student, though. The df will suffer, but the $SE_{diff}$ could be lower to counteract that. 

---
## *df*

Student's
$$df = N_1 + N_2 - 2$$
Unlike before when we subtracted by 1, we are now calculating two means, so we need to subtract by two to get the *df*

---
We calculate our test statistic with:

$$ t = \frac{\bar{X}_1-\bar{X}_2}{\sigma_D}$$ and then can find the probability of the absolute value of this test statistic (for two-tailed) or more extreme given the null is true.
---
## Examples!

Research Question: Do Kid or Adult trick R treaters get more candy
.code-small[
```{r}
set.seed(15)
trickRtreat <- data.frame("Candy"=cbind(c(round(rnorm(n=500, mean=13.256, sd = 1.243)), round(rnorm(n=35, mean=14.5742, sd = 4.4523)))), "Age"=c(rep("Kid", 500), rep("Adult", 35)))
# simulated data. How much Candy did each individual get? And are they adults or kids?
psych::describeBy(trickRtreat$Candy, group = trickRtreat$Age)
```
]


---
Calculate Pooled Variance $SE_{diff}$ (for Student's *t*-test)
```{r echo=FALSE}
psych::describeBy(trickRtreat$Candy, group = trickRtreat$Age)
```
$$  \mu= 0$$
$$\sigma_D = \sqrt{\frac{(500-1){1.28}^2 + (35-1){4.78}^2}{500+35-2}} \sqrt{\frac{1}{500} + \frac{1}{35}}$$
$$= 1.73\sqrt{\frac{1}{500} + \frac{1}{35}} = 0.30$$
$$df = 500 + 35 - 2$$ 

---

Or in R
```{r}
SE_diff <- sqrt((499*sd(trickRtreat$Candy[trickRtreat$Age=="Kid"])^2 + 34*sd(trickRtreat$Candy[trickRtreat$Age=="Adult"])^2)/533)*sqrt(1/500+1/35) 
SE_diff
```
---
With this information, we can build a sampling distribution

---

```{r}
cv <- qt(p = .975, df = 533) # critical value #.975 for a two-tailed alpha .05 test
x <- c(-5:5)
data.frame(x) %>% ggplot(aes(x=x)) + stat_function(fun= function(x) dt(x, df = 533), geom = "area", xlim = c(cv, 5), fill = "aquamarine1") + stat_function(fun= function(x) dt(x, df = 533), geom = "area", xlim = c(-5, cv*-1), fill = "aquamarine1") + stat_function(fun= function(x) dt(x, df = 533), geom = "line") + labs(x = "Difference in means", y = "density") + theme_light(base_size = 20)
```


---
We have $SE_{diff}$. Now we can divide the mean differences by $SE_{diff}$ to get student's *t* value. 
```{r}
tvalue <- (mean(trickRtreat$Candy[trickRtreat$Age=="Adult"])-mean(trickRtreat$Candy[trickRtreat$Age=="Kid"])-(0-0))/SE_diff
tvalue
cv
abs(tvalue) > abs(cv)
```

---

```{r}
x <- c(-5:5)
data.frame(x) %>% ggplot(aes(x=x)) + stat_function(fun= function(x) dt(x, df = 533), geom = "area", xlim = c(cv, 5), fill = "aquamarine1") +
  stat_function(fun= function(x) dt(x, df = 533), geom = "area",  xlim = c(-5, cv*-1), fill = "aquamarine1") + stat_function(fun= function(x) dt(x, df = 533), geom = "line") +
  labs(x = "Difference in means", y = "density") + theme_light(base_size = 20) + geom_vline(xintercept=tvalue, color = "cornflowerblue") + theme_bw()
```

Our T value exceeds the critical value cutoff, so our student's *t*-test is significant. 

---
```{r}
2*pt(q=tvalue, df = 533, lower.tail = F)
```
Can use pt to confirm this. 

---
Why stop there? We can also calculate confidence intervals of the mean difference

Confidence intervals are used to communicate the precision in how well our statistic estimates the parameter. As a reminder, they are grounded in frequentist probability: if we repeated our experiment or sampling infinitely, we would expect that 95% of our 95% confidence intervals would capture the true population parameter.

In an independent sample's t-test, we calculated three statistics, and so you can construct three different confidence intervals. yay

---
### Confidence interval around the difference in means

The most interpretable statistic is the difference in means -- this is the statistic you are testing using NHST. 

$$CI_{\text{Difference}} = (\bar{X}_1 - \bar{X}_2) \pm \sigma_D(CV)$$

```{r echo=F}
psych::describeBy(trickRtreat$Candy, group = trickRtreat$Age)
```


---
### Confidence interval around the difference in means

```{r}
SE_diff #remember this will differ depending on Welch or Student
cv
```

$CI_{\text{Difference}} = (14.09 -13.30) \pm .30(1.96)$
$$[.20, 1.38]$$

Confidence interval doesn't include zero. Significant!
---
### Confidence intervals around estimates of the mean

In addition to calculating precision of the estimate in difference in means, you may also want to calculate the precision of the mean estimates themselves. 

In this case, you should use the standard deviation of the group sample as your estimate of population sd, instead of merging them. 

$$
\begin{aligned}
CI_{\text{Mean}} &= \bar{X} \pm \sigma_M(CV) \\
 &= {\bar{X}} \pm \frac{\hat{\sigma}}{\sqrt{N}}(CV)
\end{aligned}
$$
---
.pull-left[
**Adults**

```{r}
sd(trickRtreat$Candy[trickRtreat$Age == "Adult"], na.rm=T)
qt(.975, df = 35-1)
14.09 + (4.78/sqrt(35)*2.03)
14.09 - (4.78/sqrt(35)*2.03)
```


$$14.09 \pm \frac{4.78}{\sqrt{35}}(2.03)$$
$$[12.45, 15.73]$$
]

.pull-right[
**Kids**

```{r}
sd(trickRtreat$Candy[trickRtreat$Age  == "Kid"], na.rm=T)
qt(.975, df = 500-1)
13.30 + (1.28/sqrt(500)*1.96)
13.30 - (1.28/sqrt(500)*1.96)
```


$$13.30 \pm \frac{1.28}{\sqrt{500}}(1.96)$$
$$[13.19, 13.41]$$
]





---
Or we can just use an R function for all of this of course
```{r}
t.test(trickRtreat$Candy ~ trickRtreat$Age, var.equal = T) #var.equal = T uses Student's test
```

Adults get significantly more candy trick or treating than kids as we showed before. Should I try to publish this cool result or is this really the whole story? 

---
```{r}
psych::describeBy(trickRtreat$Candy, group = trickRtreat$Age)
```

*SD* seems to be pretty unequal, and the uneven sample size is going to exacerbate this issue. Student's *t* is pretty robust to homogeneity of variance violations when sample size is equal, but that's not the case here. 
---
Why don't we make a figure to see what's going on. You can't go wrong with a bar graph. 

```{r, out.width='50%'}
plot_data <- Rmisc::summarySE(trickRtreat, "Candy", "Age")
plot_data %>% ggplot(aes(x = Age, y = Candy, fill=Age)) + geom_bar(stat = "identity") + ggtitle("amount of Candy trick or treated by age group") + geom_errorbar(aes(ymin=Candy-ci, ymax=Candy+ci)) + xlab("Age Group") +scale_y_continuous(breaks=seq(5,16,1)) +coord_cartesian( ylim = c(5,16))
```

---
You can't go wrong with a bar graph (But you can do better)
---

```{r, warning=FALSE, message=FALSE}
ggstatsplot::ggbetweenstats(data = trickRtreat, x = Age,y = Candy,xlab = "Age Group",ylab = "Candy",title = "violin plot: amount of Candy trick or treated by age group",results.subtitle = T,var.equal = T,bf.message = FALSE,mean.size=10,messages = F)
```
Almost all the variability in Kids' candy scores are contained within Adults' interquartile (middle 50%) range. Yikes.

---
```{r}
trickRtreat %>% ggplot(aes(x = Candy, fill = Age)) + geom_histogram(bins = 50, color = "white", position = "dodge") + labs(x = "Candy", y = "Frequency", title = "Candy Distribution by Age Group") + scale_fill_manual(values = c("darkorchid1", "darkturquoise")) + theme_bw(base_size = 20)
```

---
```{r}
subset(trickRtreat, trickRtreat$Age == "Adult") %>% ggplot(aes(x = Candy)) + geom_histogram(bins = 50, color = "white", fill = "darkorchid1", position = "dodge") + labs(x = "Candy", y = "Frequency", title = "Candy Distribution for Adults") + scale_fill_manual(values = c("darkorchid1")) + theme_bw(base_size = 20)
```

candy's normality isn't as bad as it first appears. 
---
Use custom colors 4 fun
http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf

---
## Welch's

This is where Welch's comes in handy (although I default to it regardless)
```{r}
t.test(trickRtreat$Candy ~ trickRtreat$Age, var.equal = F) #var.equal = F uses Welch's test, although it's also the default, so you don't need to type it
```

With Welch's test, adults do not get significantly more candy than kids on Halloween. Sad.
We can perform a significance test of homogeneity of variance as reassurance that the variances are indeed unequal. 
---
Homogeneity of variance can be checked with Levene’s procedure.  It tests the null hypothesis that the variances for two or more groups are equal (or within sampling variability of each other): 

$$H_0: \sigma^2_1 = \sigma^2_2$$
$$H_A: \sigma^2_1 \neq \sigma^2_2$$
Levene's test can be expanded to more than two variances; in this case, the alternative hypothesis is that at least one variance is different from at least one other variance.So a significant F value means a significant departure from homogeneity of variance. 

Levene's produces a statistic, *W*, that is *F* distributed with degrees of freedom of $k-1$ and $N-k$.

---

```{r}
car::leveneTest(Candy~as.factor(Age), data = trickRtreat, center = "mean")
```
Levene’s test gets more powerful as sample size increases. So unless your two variances are *exactly* equal to each other (and if they are, you don't need to do a test), your test will be "significant" with a large enough sample. Part of the analysis has to be an eyeball test -- is this "significant" because they are truly different, or because I have many subjects. 

Don't need to base decision of Welch or Student based on a significance test. Utilize some of the tools we used earlier (looking at SDs, visualizations of spread and confidence intervals)
---
### Normality

Finally, there's the assumption of normality. Specifically, this is the assumption that the population is normal -- if the population is normal, then our sampling distribution is **definitely** normal and we can use a *t*-distribution.

But even if the population is not normal, the Central Limit Theorem lets us assume our sampling distribution is normal because as N approaches infinity, the sampling distributions approaches normality. So we can be **pretty sure** the sampling distribution is normal. 

One thing we can check -- the only distribution we actually have access to -- is the sample distribution. If this is normal, then we can again be pretty sure that our population distribution is normal, and thus our sampling distribution is normal too. But again, the sample distributions aren't required to be normally distributed. 

---

Normality can be checked with a formal test: the Shapiro-Wilk test.  The test statistic, W, has an expected value of 1 under the null hypothesis. Departures from normality reduce the size of W.  
A statistically significant W is a signal that the sample distribution departs significantly from normal.

But...
* With large samples, even trivial departures will be statistically significant.
* With large samples, the sampling distribution of the mean(s) will approach normality, unless the data are very non-normally distributed.
* Visual inspection of the data can confirm if the latter is a problem.
* Visual inspection can also identify outliers that might influence the data.

---
.code-small[
```{r}
shapiro.test(x = trickRtreat$Candy)
shapiro.test(x = trickRtreat$Candy[trickRtreat$Age == "Adult"])
shapiro.test(x = trickRtreat$Candy[trickRtreat$Age == "Kid"])
```
]

It's obvious that this isn't normal because the two groups have very different variabilities. It might seem surprising that Kids' significantly deviates from normalty, but we have a lot of Kids in the sample size.  
---

A common non-parametric test (in the event of egregious non-normality) that can be used in place of the independent samples *t*-test is the **Wilcoxon sum rank test**.  

* Order all the data points by their outcome. 
* For one of the groups, add up all the ranks. That's your test statistic, *W*.
* To build the sampling distribution, randomly shuffle the group labels and add up the ranks for your group of interest again. Repeat this process until you've calculated the rank sum for every possible group assignment. 

```{r}
wilcox.test(Candy~Age, data = trickRtreat)
```


---
### New Examples
```{r}
# 249 partygoers were asked by a researcher how much fun they had at a party (HadFun) on a scale of 1 (No fun) to 21 (SO much fun)
# partygoers were also secretly rated by the researcher on how much fun they were to be around on the same scale
# the researcher noted whether the partygoer wore a costume
# Hypothesis: costume wearers would be more fun to be around but would have less fun than non-costume wearers
set.seed(15)
Partay <- data.frame("WereFun"=cbind(c(
  round(rnorm(n = 115, mean = 15.3526, sd = 1.4325)),
  round(rnorm(n = 134, mean = 16.3562, sd = 1.743)))), 
  "HadFun"=cbind(c(
    round(rnorm(n = 115, mean = 16.7436, sd = 1.643)),
    round(rnorm(n = 134, mean = 14.5464, sd = 2.4587)))),
  "Costume" = c(rep("No", 115), rep("Yes", 134))
)
t.test(WereFun ~ Costume, data = Partay) #var.equal = F
```


---

```{r}
t.test(HadFun ~ Costume, data = Partay) #var.equal = F
```


---
### Effect Sizes: Do significant differences really make a difference?

Significance isn't a great proxy for meaningfulness or size of an effect. Many factors besides effect size goes into significance (provided the effect is not 0). And eyeballing how large an effect is can be difficult unless you're aware of the sample variances. 

**Cohen's d** is a standardized mean difference and is one of the most common effect size estimate. Easy to understand and to compare across different experiments/studies. 

$$\delta = \frac{\mu_1 - \mu_0}{\sigma} \approx d = \frac{\bar{X}_1-\bar{X}_2}{\hat{\sigma}_p}$$


--

Cohen’s d is in the standard deviation (Z) metric.


Cohen's doesn't divide by *SE* so increasing sample size won't necessarily increase Cohen's D unless the effect grows or the variance decreases. 



---


```{r echo = FALSE}
stats = psych::describeBy(WereFun ~ Costume, data = Partay)
stats
```

```{r, echo = F}
x1 = stats$No[1, "mean"]
s1 = stats$No[1, "sd"]
n1 = stats$No[1, "n"]
x2 = stats$Yes[1, "mean"]
s2 = stats$Yes[1, "sd"]
n2 = stats$Yes[1, "n"]
rx1 = round(x1, 2)
rs1 = round(s1, 2)
rn1 = round(n1, 2)
rx2 = round(x2, 2)
rs2 = round(s2, 2)
rn2 = round(n2, 2)
s=sqrt(((n1-1)*s1^2+(n2-1)*s2^2)/(n1+n2-2))
s_d = s*sqrt((1/n1) + (1/n2))
df = n1+n2-2
```


.pull-left[
**Costume**
$$
\begin{aligned}
 \bar{X}_1 &= `r round(stats$Yes[1, "mean"],2)` \\
 \hat{\sigma}_1 &= `r round(stats$Yes[1, "sd"],2)` \\
 N_1 &= `r round(stats$Yes[1, "n"])` \\
\end{aligned}
$$
]

.pull-right[
**No Costume**
$$
\begin{aligned}
 \bar{X}_2 &= `r round(stats$No[1, "mean"],2)` \\
 \hat{\sigma}_2 &= `r round(stats$No[1, "sd"],2)` \\
 N_2 &= `r round(stats$No[1, "n"])` \\
\end{aligned}
$$
]

---

$$\hat{\sigma}_p = \sqrt{\frac{(`r rn1`-1){`r rs1`}^2 + (`r rn2`-1){`r rs2`}^2}{`r rn1`+`r rn2`-2}} 
 = `r round(s,2)`$$
 
```{r, echo = F}
d = (x1-x2)/s
```
 
 
$$d = \frac{`r round(x1,2)`-`r round(x2,2)`}{`r round(s,2)`} = `r round(d,2)`$$

How do we interpret this? Is this a large effect?

---

Cohen (1988) suggests the following guidelines for interpreting the size of d:

.large[

- 	.2 = Small

- 	.5 = Medium

- 	.8 = Large	

]

An aside, to calculate Cohen's D for a one-sample *t*-test: 
$$d = \frac{\bar{X}-\mu}{\hat{\sigma}}$$


.small[Cohen, J. (1988), Statistical power analysis for the behavioral sciences (2nd Ed.). Hillsdale: Lawrence Erlbaum.]

---











```{r}
# import Armand Gilles' Dataset
pokemon <- read.csv("https://gist.githubusercontent.com/armgilles/194bcff35001e7eb53a2a8b441e8b2c6/raw/92200bc0a673d5ce2110aaad4544ed6c4010f687/pokemon.csv") %>%
  subset(Generation == 1 | Generation == 2) %>%
  select(-X.) 
head(pokemon, 10)
```

---
Research Question 1: Are pokemon with one type at least 10 points weaker in total ability (stats) than pokemon with two types

```{r}
pokemon$typenumber <- ifelse(pokemon$Type.2 =="", 1, 2)
t.test(pokemon$Total ~ pokemon$typenumber, mu = -10, alternative = "less")
```
The difference is significantly greater than 10 (rather than significantly different from zero)
---
Research Question 2: Are pokemon from gen 2 (1999) stronger than gen 1 (1996)?

```{r  warning=FALSE, message=FALSE}
pokemonlong <- pokemon %>%
  pivot_longer(c(Total, HP, Attack, Defense, Sp..Atk, Sp..Def, Speed), names_to = "variables", values_to = "values")
library(rstatix)
pokemontest <- pokemonlong %>% group_by(variables) %>% t_test(values ~ Generation, detailed = T) %>% adjust_pvalue(method = 'holm') #adjust_pvalue(method = 'holm') adjusts p values for family wise error rate
# var.equal = F implied
pokemontest

```
Only speed is significant, even after holm correction, but in the opposite direction as hypothesized.


---
Can make an exportable figure with the code in the comment below
```{r, message=FALSE}
pokemonsd <- pokemonlong %>% group_by(variables, Generation)  %>% summarise(sd=format(round(sd(values, na.rm = T), 2), nsmall=2)) 
pokemonsdwide <- pokemonsd %>% pivot_wider(names_from = Generation, values_from = sd)
pokemontest2 <- data.frame("Outcome"= pokemontest$variables, "Gen1"=format(round(pokemontest$estimate1, 2), nsmall=2), "Gen2" = format(round(pokemontest$estimate2, 2), nsmall=2), 'MeanDifference'=format(round(pokemontest$estimate, 2), nsmall=2) , "conf.low"=pokemontest$conf.low <- format(round(pokemontest$conf.low, 2), nsmall=2) ,"conf.high"= format(round(pokemontest$conf.high, 2), nsmall=2), "t"=format(round(pokemontest$statistic, 2), nsmall=2) , "p"=format(round(pokemontest$p, 3), nsmall=3) , "p.holm"=format(round(pokemontest$p.adj, 3)), "df"=format(round(pokemontest$df, 2), nsmall=2) , "SD1"= pokemonsdwide$`1`, "SD2" = pokemonsdwide$`2`)
pokemontest2 <- pokemontest2 %>% 
  mutate(Outcome=factor(Outcome, levels =c("Total", "HP", "Attack", "Defense", "Sp..Atk", "Sp..Def", "Speed"))) %>% arrange(Outcome)

pokemontest2 <- pokemontest2 %>% unite("95% CI", conf.low:conf.high, sep = " ,", remove = T) 
pokemontest2 <- pokemontest2 %>% unite("Gen 1 M (SD)", c(Gen1, SD1), sep = " (", remove = T)
pokemontest2 <- pokemontest2 %>% unite("Gen 2 M (SD)", c(Gen2, SD2), sep = " (", remove = T)
pokemontest2 # write.table(pokemon2, file = "filename", sep = ",", quote = FALSE, row.names = F)

```


