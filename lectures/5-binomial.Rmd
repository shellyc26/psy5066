---
title: 'Binomial Distribution'
output:
  xaringan::moon_reader:
    includes: 
    lib_dir: libs
    css: [default, rladies, rladies-fonts, "my-theme.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide'}
library(tidyverse)
```

The **binomial distribution** is the theoretical probability distribution appropriate when modeling the expected outcome, X, of N trials (or event sequences) that have the following characteristics:

--
- The outcome on every trial is binary 

    - also called a **Bernoulli trial**

--

- The probability of the target outcome (usually called a “success”) is the same for all N trials 

--

- The outcomes of the trials are *independent*

  - The probability of a success in any one trial must be the same from trial to trial

--

- The number of trials is fixed (you know how many times you're flipping a coin)

---

If these assumptions hold then $X$ is a binomial random variable representing the *expected number of successes* over $N$ trials, with expected success on each trial of $\theta$ .

<p>&nbsp;</p>


A common and compact way of stating the same thing is: 

<p>&nbsp;</p>


$$\Huge X \sim B(N, \theta)$$
.small[
Note: Last lecture we used $p$ to denote the probability of success. This time we'll use $\theta$. $\theta$ is more correct for the population parameter, but you'll see $p$ used a lot, too.
]
---

The probability distribution for X is defined by the following **probability mass function**: 

$$\Large P(X|\theta,N) = \frac{N!}{X!(N-X)!}\theta^X(1-\theta)^{N-X}$$

The probability mass function tells us what to expect for any particular value of X in the sample space.

---

The probability distribution for X is defined by the following **probability mass function**: 

$$\Large P(X = k|n,p) = \bigg(\frac{n}{k}\bigg)p^k(q)^{n-k}$$

This is the same exact thing as previous slide! 

<p>&nbsp;</p>

--

All theoretical distributions have a mass function (if discrete) or a density function (if continuous). These are the defining equations that tells us the generating process for the behavior of X.

---

$$\Large P(X|\theta,N) = \frac{N!}{X!(N-X)!}\theta^X(1-\theta)^{N-X}$$

***

$\mathbf{P(X|\theta,N)}$ is a conditional probability: the probability of X **given** $\theta$ and $N$.

- X is the number of successful trials (in our other formula this is k -- as in r correct "choices")

- N is the number of trials; must be independent

- $\theta$ is the probability of success on any given trial (in our other formula, this is p)


$\theta$ and N are parameters of the binomial distribution.

The probability mass function tells us what to expect for any particular value of X in the sample space.

---

$$\Large P(X|\theta,N) = \frac{N!}{X!(N-X)!}\theta^X(1-\theta)^{N-X}$$

***

$\mathbf{\theta^X(1-\theta)^{N-X}}$ is the probability of any particular instance of X.  
- This is just a general form of the basic probability rule:

$$A \text{ and } B = P(A \cap B) = P(A)P(B)$$
- AKA the intersection AKA $p(success) \times p(failure)$
- Note that this form of the rule assumes *independent events*.

---

## Binomial Distribution

$$
\Large p(X=k); n, p = \binom{n}{k}p^k(1-p)^{n-k}
$$

- Parameters of this distribution are $n$ and $p$
- If we change the parameters, we change the distribution
- **Family** of distributions

---

## Family of Distributions

.pull-left[
```{r, echo = FALSE}
data.frame(heads = 0:10, prob = dbinom(x = 0:10, size = 10, prob = .5)) %>% 
  ggplot(aes(x = factor(heads), y = prob)) +
  geom_col(fill = "#562457") +
  geom_text(aes(label = round(prob, 2), y = prob + .01),
            position = position_dodge(.9),
            size = 5, 
            vjust = 0) +
  labs(title = "Binomial Distribution of Coin Flips",
       subtitle = "n = 10, p = .5",
       x = "Number of Successes (Heads)",
       y = "Probability(Successes)") +
  theme_classic(base_size = 16)
```
]

.pull-right[
```{r, echo = FALSE}
data.frame(heads = 0:10, prob = dbinom(x = 0:10, size = 10, prob = (1/6))) %>% 
  ggplot(aes(x = factor(heads), y = prob)) +
  geom_col(fill = "#562457") +
  geom_text(aes(label = round(prob, 2), y = prob + .01),
            position = position_dodge(.9),
            size = 5, 
            vjust = 0) +
  labs(title = "Binomial Distribution of Dice Rolls",
       subtitle = "n = 10, p = 1/6",
       x = "Number of Successes (Rolling a 4)",
       y = "Probability(Successes)") +
  theme_classic(base_size = 16)
```
]
---

## Parameters

Every probability distribution has an **expected value**.
  - Expected Value is essentially the average value if you repeated the experiment...a lot
  - Most likely result of the probability function
  - The thing we would expect to happen if we have no other information than the parameters of the distribution
  - The *long-run* average over an infinite amount of trials or samples

---
## Averages vs. EV

What is the average number of successes out of 5 flips? 

```{r binom-plot, echo = FALSE, out.width="50%"}
data.frame(heads = 0:5, prob = dbinom(x = 0:5, size = 5, prob = .5),
           correct = c("not", "not", "not", "yes", "not", "not")) %>% 
  ggplot(aes(x = factor(heads), y = prob)) +
  geom_col(aes(fill = correct), show.legend=FALSE) +
  geom_text(aes(label = round(prob, 2), y = prob + .01),
            position = position_dodge(.9),
            size = 5, 
            vjust = 0) +
  labs(title = "Probability of Heads",
       subtitle = "Five Flips",
       x = "Number of Successes (Heads)",
       y = "Probability(Successes)") +
  theme_classic(base_size = 16) +
  scale_fill_manual(values = c("#562457","#4BB149"))
```

---

## Parameters


Every probability distribution has an **expected value**. 

For the binomial distribution:

$$E(X) = N\theta$$
For 3 Heads out of 5 flips, $E(X) = 5 \times .5 = 2.5$

--

Each probability distribution also has a variance. For the binomial:

$$Var(X) = N\theta(1-\theta)$$
--

Importantly, this means our mean and variance are related in the binomial distribution, because they both depend on $\theta$. How are they related?

---

## Let's set our mean = variance


$$ 
\begin{equation}
N\theta = N\theta(1-\theta) \\
Np = Np(1-p) \\
Np = Np(q) \\
\text{mean} = \text{mean} \times q
\end{equation}
$$


--

- If q = 1, then $E(X) = Var(X)$ 
- Else, $E(X) > Var(X)$ 

---

.left-column[
The mean, 2.5, does not exist in the sample space, and rounding up to 3 and claiming that to be the most typical outcome is not quite right either.
]

```{r ref.label = 'binom-plot', echo = FALSE}

```

- If you have a discrete distribution with a small N, these estimates may not have a sensible meaning
---

The **probability mass (density) function** allows us to answer other questions about the sample space that might be more important, or at least realistic.
- mass = discrete
- density = continuous

--

I might want to know the value in the sample space at or below which a certain proportion of outcomes fall.
 > "At or below what outcome in the sample space do .75 of the outcomes fall?"

This is a **percentile or quantile** question. 

--

I might want to know the proportion of outcomes in the sample space that fall at or below a particular outcome. This is a **cumulative proportion** question.

---
At or below what outcome in the sample space do 75% of the outcomes fall? What is the outcome? Providing the `75%`, trying to find the `2`...

```{r binom-plot2, echo = FALSE}
data.frame(heads = 0:10, prob = dbinom(x = 0:10, size = 10, prob = (1/6))) %>% 
  ggplot(aes(x = heads, y = prob)) +
  geom_col(fill = "#562457") +
  geom_vline(aes(xintercept = 2),
             color = "#4BB149") +
  labs(title = "Probability Mass Distribution of Dice Rolls",
       subtitle = "n = 10, p = 1/6",
       x = "Number of Successes (Rolling a 4)",
       y = "Probability(Successes)") +
  theme_classic(base_size = 16) +
  scale_x_continuous(breaks = c(0:10))
``` 

---
What proportion of outcomes in the sample space that fall at or below a given outcome? What percentage of outcomes fall at or below 2? Providing the `2`, looking for the `75%`

.pull-left[
```{r ref.label = 'binom-plot2', echo = FALSE}

```

]
.pull-right[
```{r echo = FALSE}
data.frame(num = 0:10, p = pbinom(q = 0:10, size = 10, prob = 1/6)) %>% ggplot(aes(x=num, y=p)) + geom_bar(stat="identity", fill = "#562457") + 
  labs(title = "Cumulative Distribution of Dice Rolls",
       subtitle = "n = 10, p = 1/6",
       x = "Number of Successes (Rolling a 4) in 10 rolls (N)",
       y = "Cumulative Proportion") +
  theme_classic(base_size = 16) +
  scale_x_continuous(breaks = c(0:10))
``` 
]

---

The binomial is of interest beyond describing the behavior of dice and coins.

Many practical outcomes might be best described by a binomial distribution.

For example, suppose I give a 40-item multiple choice test, with each question having 4 options.

* I am worried that students might do well by chance alone.  I would not want to pass students in the class if they were just showing up for the exams and guessing for each question.

--

* What are the parameters in the binomial distribution that will help me address this question?
  - $N = 40$
  - $\theta = .25$

---

```{r binom-plot3, echo = F}
limit = 30
data.frame(num = 0:limit, p = dbinom(x = 0:limit, size = 40, prob = .25)) %>% ggplot(aes(x=num, y=p)) + geom_bar(stat="identity") + scale_x_continuous("Test Score (0-40) Under a Guessing Model", breaks=seq(0,limit,2)) +scale_y_continuous("Probability")+ggtitle("Binomial Probability Distribution")
```

???
I could use this distribution to help me decide if a given student is consistent with a guessing model.

Nearly all of the outcomes expected for guessers fall below the minimum passing score (60%, D-, 24).

---

How likely is it that a guesser would score above the threshold (60%) necessary to pass the class by the most minimal standards?

$$P(24|.25, 40) + P(25|.25,40) + P(26|.25,40) + ... + P(40|.25, 40) $$

--

In R, we can calculate the cumulative probability (X or lower), using the `pbinom` function.

The probability of getting 23 questions or fewer correct:

```{r}
pbinom(q = 23, size = 40, prob = .25)
```

--

The probability of getting 24 or more questions correct:
```{r}
#Note the use of the Law of Total Probability here

1-pbinom(q = 23, size = 40, prob = .25)
```

---

Cumulatively, what proportion of guessers will fall below each score?

```{r, echo = F}
limit = 40
data.frame(num = 0:limit, p = pbinom(q = 0:limit, size = 40, prob = .25)) %>% ggplot(aes(x=num, y=p)) + geom_line() + geom_vline(aes(xintercept = 24), color = "red") + scale_x_continuous("Test Score (0-40) Under a Guessing Model", breaks=seq(0,limit,2)) +scale_y_continuous("Cumulative Proportion")+ggtitle("Cumulative Binomial Probability Distribution")
```

???

Seems safe to assume that, practically speaking, all guessers will fall below the minimally passing score.

---
## There’s always a but

But, what assumptions are we making and what consequences will they have?


* The outcome on every trial is binary (also called a Bernoulli trial)
* The probability of the target outcome (usually called a "success") is the same for all N trials
* The trials are independent $P(A\cap B) = P(A|B)P(B)=P(A)P(B)$
* The number of trials is fixed


In probability and statistics, if the assumptions are wrong then inferences based on those assumptions could be wrong too, perhaps seriously so.

---

> All models are wrong, but some models are useful.  (G.E.P. Box)

We might have viable alternative models:

* **Geometric distribution:**  Used if we are interested in the number of trials required for one "success" to occur 
  * "how many times do I start my computer before it fails to start at all?"
  

* **Negative binomial distribution:** Used if we are interested in the number of successes in a series of repeated trials until a specified number of failures are seen 
  * "A child won't return from trick or treating until they get 5 full-size candy bars. What is the probability that they will have to visit 34 homes to get this?"

---

.left-column[

.small[As N increases, the binomial becomes more normal in appearance.

Because of the difficulties in calculating large factorials, there is a large-sample normal approximation to the binomial. The normal distribution is useful for a lot of other reasons too.
]
]


```{r, echo = F}
data.frame(num = 0:30, p = dbinom(x = 0:30, size = 100, prob = 1/6)) %>% ggplot(aes(x=num, y=p)) + geom_bar(stat="identity") + scale_x_continuous("Number of sixes (X) in 100 rolls (N)", breaks=seq(0,30,2)) +scale_y_continuous("Probability")+ggtitle("Binomial Probability Distribution")
```

---

class: inverse

## Next time...

the normal distribution