<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Threats to validity</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs-2.27/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="my-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Threats to validity
]

---



## Why statistics

- An essential aid to “signal detection”

- A universal language for communicating what we find.

- Required for competent evaluation of others’ work.

---

class: inverse
## Goal of today

Advanced skill in quantitative methods carries with it the responsibility to use those skills carefully and ethically.

Today, we'll discuss methodological issues present in statistics. 
- It can be tempting to use statistics to fix poor research design.
- *These issues cannot be fixed quantitatively* (even when it looks like they can).

- *Do you believe any of the statistics you ran all semester?*


---

## Constructs

- Our basic goal in science is to make inferences about the causal relations between constructs.
	
![](images/intro_construct.jpg)

--

- We can’t do that directly, so we rely on proxies for those constructs

&gt; We are measuring the invisible
---

### Measuring the Invisible 

![](images/intro_operation.jpg)

In order to infer that A --&gt; B, we have to make three assumptions:
 - X is a good proxy for A
 - Y is a good proxy for B
 - X and Y are causally related
 
---

## Measuring the Invisible

- When the first two assumptions are true, the relation between X and Y will provide a good estimate of the relation between A and B.

- What threatens our ability to carry out this seemingly simple task? 
    - How do quantitative methods help us solve these problems?

---
class: middle, inverse, center

# Do you believe it?

---

## Validity

Four kinds of validity in research threaten our ability to make valid causal inferences. Solving each problem either directly requires quantitative methods or makes use of principles that are central to quantitative methods.


- Statistical conclusion validity

- Internal validity

- Construct validity

- External validity

---

## Statistical conclusion validity

- **Definition:** the validity of the inference that X and Y are related (aka that you are making appropriate and reasonable conclusions). How well do the numbers support the claim? Do you believe the numbers or do you think they are lying to you?

![](images/intro_operation.jpg)

---

### (Some) threats to statistical conclusion validity

.pull-left[
- low statistical power

- violations of assumptions 

- fishing and the error rate problem

- unreliable measures

- restricted range

- unreliable treatment implementation
]

---

### (Some) threats to statistical conclusion validity

.pull-left[
- **low statistical power**

- violations of assumptions 

- fishing and the error rate problem

- unreliable measures

- restricted range

- unreliable treatment implementation
]

.pull-right[
Power = ability to detect an effect, if one is there
]

---

### (Some) threats to statistical conclusion validity

.pull-left[
- low statistical power

- **violations of assumptions** 

- fishing and the error rate problem

- unreliable measures

- restricted range

- unreliable treatment implementation
]

.pull-right[
Statistical tests have assumptions
  - If we violate those assumptions, is that a useful test?
]

---

### (Some) threats to statistical conclusion validity

.pull-left[
- low statistical power

- violations of assumptions 

- **fishing and the error rate problem**

- unreliable measures

- restricted range

- unreliable treatment implementation
]

.pull-right[
Independence Assumption
  - We're OK with 5/100 or 1/20
  - But if you do 20 tests on the same data...
]

---

### (Some) threats to statistical conclusion validity

.pull-left[
- low statistical power

- violations of assumptions 

- fishing and the error rate problem

- **unreliable measures**

- restricted range

- unreliable treatment implementation
]

.pull-right[
Reliability = consistency
]

---

### (Some) threats to statistical conclusion validity

.pull-left[
- low statistical power

- violations of assumptions 

- fishing and the error rate problem

- unreliable measures

- **restricted range**

- unreliable treatment implementation
]

.pull-right[
How do you know if there's actually a relationship, when you neglect real values? 
]

---

### (Some) threats to statistical conclusion validity

.pull-left[
- low statistical power

- violations of assumptions 

- fishing and the error rate problem

- unreliable measures

- restricted range

- unreliable treatment implementation/group assignment
]

.pull-right[
If you say you randomly assigned participants into groups, that better be the case...
]
---

## Internal validity

- **Definition:** the validity of the inference that X and Y are _**causally**_ related. There are no other possible explanations

  - Given that X and Y are correlated, can we validly infer that the relation is causal?

--

- Requirements:
  
  - Temporal precedence
  - No confounds

???

direction = Cause precedes effect

correlation = Cause covaries with effect

confounds = Other explanations are ruled less plausible

---

### Threats to internal validity

.pull-left[
- ambiguous temporal precedence

- selection

- attrition

- history

- maturation

- regression

- testing

- instrumentation
]

---

### Threats to internal validity

.pull-left[
- **ambiguous temporal precedence**

- selection

- attrition

- history

- maturation

- regression

- testing

- instrumentation
]

.pull-right[

Temporal precedence can be established in an experiment because treatment precedes outcome. 

But, when treatment is not possible, then logic and common sense can sometimes dictate temporal precedence.

- prenatal nutrition and cognitive development

- depression and cancer
]

---

### Threats to internal validity

.pull-left[
- ambiguous temporal precedence

- **selection**

- attrition

- history

- maturation

- regression

- testing

- instrumentation
]

.pull-right[

Any systematic differences between groups that might account for an observed effect.

- Test scores of students who visit the Psychology tutoring center vs students who do not visit tutoring center.

How to combat this?
]

???

Combat this with random assignment.

---

### Threats to internal validity

.pull-left[
- ambiguous temporal precedence

- selection

- **attrition**

- history

- maturation

- regression

- testing

- instrumentation
]

.pull-right[

Even if random assignment is used, participants may drop out of the study, producing unequal groups, a situation that has the same inferential problems as selection.

]

---

### Threats to internal validity

.pull-left[
- ambiguous temporal precedence

- selection

- attrition

- **history**

- maturation

- regression

- testing

- instrumentation
]

.pull-right[

.small[History refers to any event that occurs between the beginning of treatment and the measurement of outcome that might have produced the observed effect.

- A marketing campaign intended to increase beer sales happens to coincide with other events that might have the same effect: a particularly hot period of weather, a long losing streak by the St. Louis Cardinals, etc.]

]

---

### Threats to internal validity

.pull-left[
- ambiguous temporal precedence

- selection

- attrition

- history

- **maturation**

- regression

- testing

- instrumentation
]

.pull-right[

Maturation refers to changes in the organism that occur regardless of treatment and that may masquerade as a treatment effect.

- A school-wide educational intervention to increase achievement test scores.  The entire school must get the same curriculum, so a control group in the school is not possible.
]

---

### Threats to internal validity

.pull-left[
- ambiguous temporal precedence

- selection

- attrition

- history

- maturation

- **regression**

- testing

- instrumentation
]

.pull-right[

Regression (to the mean) occurs when participants are selected because of their extreme scores and those scores are unreliable. The scores will regress toward the mean at the second assessment

- *Sports Illustrated* cover jinx
- Tall men father not-so-tall sons (Galton)

]

---

### Threats to internal validity

.pull-left[
- ambiguous temporal precedence

- selection

- attrition

- history

- maturation

- regression

- **testing**

- instrumentation
]

.pull-right[

Testing refers to the possible change that may occur just because participants have been previously measured. These are often called practice or fatigue effects.

- Students do better on the first half of test compared to the second.
- Students do better in the second half of the term compared to the first. 

]

---

### Threats to internal validity

.pull-left[
- ambiguous temporal precedence

- selection

- attrition

- history

- maturation

- regression

- testing

- **instrumentation**
]

.pull-right[

Change may occur because the measurement changes over time, perhaps becoming more or less reliable. 

Instrumentation reflects changes in the measurement; testing reflects changes in the object of measurement.

"When a measure becomes a target, it ceases to be a good measure." (Goodhart)
]

---

![](images/goodhart.jpg)

---
class: middle

![](images/chatgpt.png)
---

class: middle

The key point with internal validity is that something else besides the treatment is a plausible alternative explanation for any apparent treatment effect. 

Solving threats to internal validity is a **research design problem**, not a statistics problem.  Nonetheless, quantitative methods play a key role in making the case for internal validity.

_**Causal**_ is the "C" word. You better be realllllly sure you mean it before you use it.

---

### Removing the influence of other variables

If the "other variables" can be measured, their influence can be statistically controlled so that the hypothesized relation can be detected more accurately.

However:

**Statistical control should best be thought of as a method of last resort, to be used when design controls are not available or have failed.**


---

## Construct validity

![](images/cronbach.png)
???

##Apologize for section of article that suggests a link between "homosexual signs" and pathology -- that's just gross

##Questions: 

**1. What is construct validity?** -- Does this variable measure what it says it's going to measure?

**2. How do we determine construct validity?** -- Show that the variable correlates with all of the things it should correlate with and none of the things it shouldn't

**3. How do we figure out what it should correlate with?** -- Nomological net(work); theory
---

## Construct validity

- **Definition**: The validity of the inference that a given operationalization of of a construct does a good job representing the construct. 

![](images/intro_operation.jpg)

---

# Construct Validity

- **Construct validity** refers to the correctness of the label that is applied to the operation.

- It depends on first demonstrating adequate reliability and then is bolstered by demonstrating relations of the target operation to other operations.

???
Example of happiness survey with big fancy words actually just measuring intelligence
---
## How do you establish construct validity?

Show that the variable correlates with all of the things it should correlate with and none of the things it shouldn't

- Convergent validity

- Divergent/discriminant validity

--

How do we figure out what it should correlate with?

- **theory**

---
### Threats to construct validity

- **inadequate explication of constructs**

- **construct confounding**

- confounding constructs with levels of constructs

- reactive self-report changes

- reactivity to the experimental situation

- experimenter expectancy

- novelty and disruption effects

---
### Construct confounding

.pull-left[Operations usually tap more than one construct. Failure to recognize the full set of constructs embedded in the operation can lead to incorrect inferences about the constructs that are active.

A self-report of optimism might also reflect self-esteem or positive affect.  How can quantitative methods help?


]

.pull-right[
![](images/confounding.jpg)
]





---

## External validity 

- **Definition:** The validity of the inference that a causal relation between operations *generalizes* to other units, treatments, observations, or settings.

![](images/intro_operation.jpg)
---
### Threats to external validity

.pull-left[
- sampling bias

- experimenter effects

- Hawthorne effect

- testing effects

- situation effects
]

---
### Threats to external validity

.pull-left[
- **sampling bias**

- experimenter effects

- Hawthorne effect

- testing effects

- situation effects
]

.pull-right[
We can't study an entire population, so instead we take samples. If your sample is not representative of the population however, then how on Earth can you generalize to that population?
]

---
### Threats to external validity

.pull-left[
- sampling bias

- **experimenter effects**

- Hawthorne effect

- testing effects

- situation effects
]

.pull-right[
What if I told all my research participants that my job depended on the outcome of the study they are in? The participants might change their behaviors. And their behavior is what we are studying. So anything I find might not be generalizable (even if unintentional). Need to remain neutral!
]

---

### Threats to external validity

.pull-left[
- sampling bias

- experimenter effects

- **Hawthorne effect**

- testing effects

- situation effects
]

.pull-right[
The fact that people know they are being observed is enough to change behavior.

Ex: if someone is in a study about stress, and they know they are in the study, they may make themselves seem more stressed than they actually are. Surveys with scores reflecting higher levels of stress than what they might fill out if they didn't think they were being watched.
]

---

### Threats to external validity

.pull-left[
- sampling bias

- experimenter effects

- Hawthorne effect

- **testing effects**

- situation effects
]

.pull-right[
Testing effects are especially critical in pre/post designs. At the pre-test, they are nervous. But at the post-test, they know what to expect and are less anxious etc. Really problematic when studying something like, say, anxiety.
]

---
### Threats to external validity

.pull-left[
- sampling bias

- experimenter effects

- Hawthorne effect

- testing effects

- **situation effects**
]

.pull-right[
Situation effects can be things like time of day, the setting of the experiment/location etc. 

What if you study recall memory. You have all your participants come in before 10am. You find an effect. 

Now you repeat the study, but you have all your participants come in after 8pm. You don't find an effect. 
]
---

## Your Ethical Duty as a Scientist

Advanced skill in quantitative methods carries with it the responsibility to use those skills **carefully and ethically**.

- Know the shortcomings of your study
- REPORT the shortcomings of your study
- Let your readers understand the limitations
- Do NOT overstate your findings (or let the press overstate them)

---

class: inverse

## Next time

Exam 
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
