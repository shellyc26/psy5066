---
title: 'Normal Distribution'
output:
  xaringan::moon_reader:
    includes: 
    lib_dir: libs
    css: [default, rladies, rladies-fonts, "my-theme.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide'}
library(tidyverse)
library(ggpubr)
library(knitr)
colors = RColorBrewer::brewer.pal(4, "Set2")
```


The **normal distribution** ("bell curve" or "Gaussian distribution") is a two-parameter distribution defined by the mean (
$\mu$
) and standard deviation (
$\sigma$
) and having the following probability density function:

$$p(X|\mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma}}exp[-\frac{(X-\mu)^2}{2\sigma^2}]$$
$$X\sim N(\mu,\sigma)$$
???

Note p not P

### Also note function of Z-scores
pull out 2, pull out 1/2

---

### Expected value
The expected value of the normal distribution is quite easy.

$$E(X) = \mu$$

$$Var(X) = \sigma^2$$



---

.left-column[

.small[The **probability density function** gives the height of the curve at a particular value for X. 

Although these values communicate information about probability or likelihood, they are not probabilities.
]
]


```{r normal, echo = F, fig.height = 6}
ggplot(data.frame(x = seq(-4, 4)), aes(x)) +
  stat_function(fun = function(x) dnorm(x)) +
  scale_x_continuous("Variable X") +
  scale_y_continuous("Density")+
  ggtitle("The normal curve") +
  theme(text = element_text(size = 20))
```

```{r ref.label = "normal", eval=F}

```

---

Both the normal distribution and the binomial distribution follow the Law of Total Probability, but in different ways.

In the *binomial distribution*, each outcome in the sample space has a probability and these probabilities sum to 1.

In the *normal distribution*, there are an infinite number of values, each having a probability of 0, but the probability that some value will occur is 1.  The area under the curve is 1.

The density values are derived to insure that the area under the density curve is 1.  They have no inherent meaning beyond that.

---

Density is mass per volume. In this context (curve in two dimensions) density is mass per area.  The total density in the normal curve is 1. 

The density for a part of the curve (a smaller area) will necessarily be less than 1.

---

.left-column[
The area under the curve that lies between the mean (here 0) and a value of 1 is the probability of a score between 0 and 1.

As we shrink that area by moving X closer to the mean, that probability interpretation holds.

]

```{r, echo = F}
ggplot(data.frame(x = seq(-4, 4)), aes(x)) +
  stat_function(fun = function(x) dnorm(x)) + 
  stat_function(fun = function(x) dnorm(x) , 
                xlim = c(0, 1),
                geom = "area", fill = colors[1]) + 
  geom_hline(aes(yintercept = 0))+
  scale_x_continuous("Variable X") + scale_y_continuous("Density")+ggtitle("The normal curve") + theme(text = element_text(size =20))
```

---

.left-column[

As our interval shrinks closer and closer to 0, our area (probability) shrinks as well.

It can get vanishingly close to 0—essentially a point rather than an area.

The probability of that "point" is 0.

]

```{r, echo = F}
ggplot(data.frame(x = seq(0, 1)), aes(x)) +
  stat_function(fun = function(x) pnorm(x)-.5, color = "red") + 
  geom_hline(aes(yintercept = 0), color = "red", linetype = "dashed")+
  scale_x_continuous("Z") +
  scale_y_continuous("Density", limits=c(0,.4)) +ggtitle("Area under the normal curve between Z and 0")+ theme(text = element_text(size = 20))

```

---

.left-column[

We can keep shrinking the distance between Z and 0, never reaching 0, and still calculate an area.

It will be very, very small. 


]

```{r, echo = F}
ggplot(data.frame(x = seq(0, .001, by =.00005)), aes(x)) +
  stat_function(fun = function(x) pnorm(x)-.5, color = "red") + 
  geom_hline(aes(yintercept = 0), color = "red", linetype = "dashed")+
  scale_x_continuous("Z") +
  scale_y_continuous("Density") +ggtitle("Area under the normal curve\nbetween Z and 0")+ theme(text = element_text(size = 20))

```

???

## Key point

It does not make sense to talk about the probability of a point for a continuous variable, but it does make sense to talk about the probability that a score will fall between two points or the probability that a score will fall above or below a point.

---

## Characteristics of the normal distribution

* The mean and standard deviation are independent
* The distribution is unimodal and symmetrical.
* For two normal distributions, the area under the curve between corresponding locations in standard deviation units is the same regardless of $\mu$ and $\sigma$. 

  * For example, 68.3% of the area under a normal curve falls between 1 $\sigma$ below the mean and 1 $\sigma$ above mean—for every normal curve.  This general characteristic is called the **Empirical Rule**.

---

```{r, echo = F}
ggplot(data.frame(x = seq(-4, 4)), aes(x)) +
  stat_function(fun = function(x) dnorm(x, mean = .5, sd = 2)) +
  stat_function(fun = function(x) dnorm(x, mean = 2, sd = .2), color = "blue") +
  stat_function(fun = function(x) dnorm(x, mean = -1.25, sd = 1), color = "red") +
  scale_x_continuous("Variable X") +
  scale_y_continuous("Density") +
  ggtitle("Normal Curves")
```

---
All of these distributions are normal and have an equivalent area (proportion) that falls between a standard deviation below and above their respective means.

```{r, echo = F, fig.width=10, fig.height = 5}
p1 = ggplot(data.frame(x = seq(-4, 4)), aes(x)) +
stat_function(fun = function(x) dnorm(x) , 
                xlim = c(-1, 1),
                geom = "area", fill = colors[1]) +
  stat_function(fun = function(x) dnorm(x)) +
  geom_hline(aes(yintercept = 0)) +
  scale_x_continuous("Variable X") +
  scale_y_continuous("Density")+
  ggtitle("Area under the normal curve")

p2 = ggplot(data.frame(x = seq(-4, 4)), aes(x)) +
stat_function(fun = function(x) dnorm(x, sd = 1.5) , 
                xlim = c(-1.5, 1.5),
                geom = "area", fill = colors[1]) +
  stat_function(fun = function(x) dnorm(x, sd = 1.5)) +
  geom_hline(aes(yintercept = 0)) +
  scale_x_continuous("Variable X") +
  scale_y_continuous("Density")+
  ggtitle("Area under the normal curve")

p3 = ggplot(data.frame(x = seq(-4, 4)), aes(x)) +
stat_function(fun = function(x) dnorm(x, sd = .5) , 
                xlim = c(-.5, .5),
                geom = "area", fill = colors[1]) +
  stat_function(fun = function(x) dnorm(x, sd = .5)) +
  geom_hline(aes(yintercept = 0)) +
  scale_x_continuous("Variable X") +
  scale_y_continuous("Density")+
  ggtitle("Area under the normal curve")

ggarrange(p1, p2, p3, ncol = 3)
```

---

Approximately 68% of the data in a normal distribution will be within one standard deviation of the mean.  
- About 95% of the data will be within two standard deviations of the mean.  
- About 99.7% of the data will be within three standard deviations of the mean.

In other words, nearly all of the data will fall within 3 standard deviations of the mean in a normal distribution.

These benchmarks are convenient for determining if a score (and later, a mean) is rare or unusual in the context of a particular distribution. 

---
## Standard normal distribution

The normal distribution with $\mu$=0 and $\sigma$=1 is called standard normal.  

Variables with quite different means and standard deviations can be standardized so that they can be compared in the same metric (standard deviation units). This allows statements such as "relative to the mean, I am more conscientious (e.g., $Z=2$) than I am extraverted (e.g., $Z=1$)."

I could not say, however, that I am twice as conscientious as I am extraverted.

--

All continuous distributions can be standardized, but if they are not normal to begin with, standardization will not make them so.  *Standardization does not alter distribution shape.*


???

Why?

- not ratio scales, no set amount to compare
- not the same units -- what I'm twice as heavy as I am tall?
- Z-score is comparison to other people

---

### Standard normal distribution

There is only one (1) standard normal distribution. 
  
**How is this useful?**
  - Given any score, we can calculate the probability of getting a value greater than that z-score. *(Or less than that z-score.)*
  
  - Given any two z-scores, we can calculate the probability of getting a value between these scores. *(Or outside those z-scores)*
  
  - Given a probability $p$, we can identify the z-score at which the proportion of scores below *(or above)* $p$ falls. 
  
  - Given a probability $p$, we can identify the z-score at which the proportion of scores that fall above $-Z$ and below $Z$ is equal to $p$.
  
---

## Standardized scores (z-scores)

$$ z = \frac{x_i - M}{s} $$
Scores interpreted as distance from the mean, in standard deviations. 

### Properties of z-scores

- $\Large \Sigma z = 0$
- $\Large \Sigma z^2 = N$
- $\Large s_z = \frac{\Sigma z^2}{n}$

---

## Standardized scores (z-scores)

$$ z = \frac{x_i - M}{s} $$
**Why is this useful?**

- Compare across scales and unit of measures

- More easily identify extreme data

- mean = 0, s = 1!

???

Note for the standard deviation property

$$s_z = \frac{\Sigma z^2}{n} = \frac{n}{n} = 1$$

---

## Using z-scores

.pull-left[
```{r echo=FALSE}
kable(starwars[1:6,1:2])
```
]

.pull-right[
```{r}
starwars %>% 
  select(1:2) %>% 
  mutate_at(2, ~round(x = scale(.), digits = 2)) %>% 
  head(.) %>% 
  kable()
```

]

## Using z-scores
Given any score, we can calculate the probability of getting a value greater than that z-score. *(Or less than that z-score.)*

Check out this [z-table](https://www.dummies.com/education/math/statistics/how-to-use-the-z-table/)

  - Left column is the z-score to the tenths place in the decimal
  - Top is how you can add on to the hundreths place in the decimal
  
Luke Skywalker's height is z = -.07

